/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
training dataset size: 90
test dataset size: 10
======== Log a few random samples from the processed training set ========
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are the worldâ€™s leading expert in survival analysis. From a discharge summary, extract Chief Complaint, Physical Exam, and Admission Labs and produce one sentence. The sentence will be used for hazard calculation, so be precise, clinically accurate, and concise.<|eot_id|><|start_header_id|>user<|end_header_id|>

Please summarize the following discharge summary in one sentence focusing on Chief Complaint, Physical Exam, and Admission Labs.
 
Name:  ___                     Unit No:   ___
 
Admission Date:  ___              Discharge Date:   ___
 
Date of Birth:  ___             Sex:   F
 
Service: NEUROSURGERY
 
Allergies: 
Sulfa (Sulfonamide Antibiotics) / Sulfa (Sulfonamide 
Antibiotics) / vicodan
 
Attending: ___.
 
Chief Complaint:
Left ___ aneurysm 
 
Major Surgical or Invasive Procedure:
___ L craniotomy for ___ aneurysm clipping

 
History of Present Illness:
___ year old female with known ___ aneurysm presents for 
elective craniotomy for aneurysm clipping 
 
Social History:
___
Family History:
Mother died from cerebral aneurysm 
 
Physical Exam:
Upon discharge: 
AAO x 3, PERRL, visual fields full, no pronator drift.  MAE with 
full strength throughout.  Sensation intact.  
 
Pertinent Results:
___ ECG
Marked sinus arrhythmia. Non-diagnostic inferior Q waves. Minor 
repolarization abnormalities. Compared to the previous tracing 
of ___ sinus arrhythmia is now seen. QTc interval is longer. 
Clinical correlation is suggested.  
  
  Intervals Axes 
Rate PR QRS QT/QTc P QRS T 
79 156 88 ___ 62 
 

 
Brief Hospital Course:
On ___, Ms. ___ was admitted to the neurosurgical service for 
elective surgery. She was taken to the OR for a L craniotomy and 
___ aneurysm clipping. The procedure was well tolerated. For 
more details of the procedure please see the OP note seperately 
dictated in OMR. 

On ___, Ms. ___ was tolerating a diet and ambulating. The 
foley catheter was discontinued; the A-line was discontinued. It 
was determined she would be transferred to the floor. 

On ___, Ms. ___ was discharged home.  She was afebrile, 
hemodynamically and neurologically intact.  Per discharge 
instructions, the patient was instructed to follow up for a 
wound check and general post-operative follow up with Dr. 
___.  In the meantime, she was given prescriptions for a 
6-day medrol dosepack, muscle relaxant and oxycodone (as needed 
for pain).
 
Medications on Admission:
None
 
Discharge Medications:
1. Acetaminophen 325-650 mg PO Q6H:PRN fever or pain 
2. Docusate Sodium 100 mg PO BID 
3. OxycoDONE (Immediate Release)  ___ mg PO Q4H:PRN pain 
RX *oxycodone 5 mg ___ capsule(s) by mouth every four (4) hours 
Disp #*30 Capsule Refills:*0
4. Dexamethasone 4 mg IV Q6H 
RX *dexamethasone [DexPak 6 Day] 1.5 mg (21 tabs) 1 tablets(s) 
by mouth as directed Disp #*1 Dose Pack Refills:*0
5. Cyclobenzaprine 10 mg PO TID:PRN muscle spasms 
RX *cyclobenzaprine 5 mg 1 tablet(s) by mouth three times a day 
Disp #*25 Tablet Refills:*0

 
Discharge Disposition:
Home
 
Discharge Diagnosis:
Left ___ aneurysm 

 
Discharge Condition:
Mental Status: Clear and coherent.
Level of Consciousness: Alert and interactive.
Activity Status: Ambulatory - Independent.
 
Discharge Instructions:
Call your neurosurgeoní˜ªs office and speak to the Nurse 
Practitioner if you experience:

-Any neurological issues, such as change in vision, speech or 
movement

-Swelling, drainage, or redness of your incision

-Any problems with medications, such as nausea vomiting or 
lethargy

-Fever greater than 101.5 degrees Fahrenheit

-Headaches not relieved with prescribed medications

Activity:

-Start to resume all activities as you tolerate í˜¯ but start 
slowly and increase at your own pace.

-Do not operate any motorized vehicle for at least 10 days 
after your surgery í˜¯ your Nurse Practitioner can give you more 
detail at the time of your suture removal.  

Incision Care:

-Keep your wound clean and dry.

-Do not use shampoo until your sutures are removed. 

-When you are allowed to shampoo your hair, let the shampoo run 
off the incision line.  Gently pad the incision with a towel to 
dry.

-Do not rub, scrub, scratch, or pick at any scabs on the 
incision line.

-You need your sutures removed 7 to 10 days after surgery

Post-Operative Experiences: Physical

-Jaw pain on the same side as your surgery; this goes away 
after about a month

-You may experience constipation.  Constipation can be 
prevented by:

oDrinking plenty of fluids
oIncreasing fiber in your diet by eating vegetables, prunes, 
fiber rich breads and cereals, or fiber supplements
oExercising
oUsing over-the-counter bowel stimulants or laxatives as 
needed, stopping usage if you experience loose bowel movements 
or diarrhea

-Fatigue which will slowly resolve over time

-Numbness or tingling in the area of the incision; this can 
take weeks or months to fully resolve

-Muffled hearing in the ear near the incision area

-Low back pain or shooting pain down the leg which can resolve 
with increased activity

Post-Operative Experiences: Emotional

-You may experience depression.  Symptoms of depression can 
include

oFeeling í˜«downí˜¬ or sad
oIrritability, frustration, and confusion
oDistractibility
oLower Self-Esteem/Relationship Challenges
oInsomnia
oLoneliness

-If you experience these symptoms, you can contact your Primary 
Care Provider who can make a referral to a Psychologist or 
Psychiatrist

-You can also seek out a local Brain Aneurysm Support Group in 
your area through the Brain Aneurysm Foundation

oMore information can be found at ___

 
Followup Instructions:
___
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

A female patient presented with left cerebral aneurysm; on physical examination, BP NA, HR 79, RR NA, and Temp NA; admission labs showed WBC NA, RBC NA, Hgb NA, Hct NA, Plt NA, MCV NA, MCH NA, MCHC NA, RDW NA, and Glucose NA; the most diagnostically relevant finding was a left intracranial aneurysm confirmed and surgically clipped.<|eot_id|>
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are the worldâ€™s leading expert in survival analysis. From a discharge summary, extract Chief Complaint, Physical Exam, and Admission Labs and produce one sentence. The sentence will be used for hazard calculation, so be precise, clinically accurate, and concise.<|eot_id|><|start_header_id|>user<|end_header_id|>

Please summarize the following discharge summary in one sentence focusing on Chief Complaint, Physical Exam, and Admission Labs.
 
Name:  ___             ___ No:   ___
 
Admission Date:  ___              Discharge Date:   ___
 
Date of Birth:  ___             Sex:   F
 
Service: MEDICINE
 
Allergies: 
Iodine; Iodine Containing / Levaquin / Depakote
 
Attending: ___.
 
Chief Complaint:
Worsening hip pain 
 
Major Surgical or Invasive Procedure:
EGD

 
History of Present Illness:
___ F with SLE, CNS involvement, seizure disorder, GERD, HTN, 
migraines who presents with worsening hip pain (L>R). Patient 
was evaluated in the ED on ___ and was discharged with 
percocet. She reports a sharp deep pain in the groin, radiating 
to the lower back and buttocks. She now reports difficulty with 
walking which brought her to the ED today. She initially was 
taking the percocet for pain given from the ED but this caused 
severe nausea and she was unable to keep any of her regular 
medications down including her Tegretol. Her son then noted some 
seizure activity and she awoke lying on the stairs. She reports 
approximately ___ yr of seizure activity, mainly manifesting as 
LOC, occasional tongue biting, she has also been incontinent in 
the past. She has lost her job as a ___ due to passing 
out and missing work due to doctor's appointments. She also 
reports multiple other areas of joint pain including her 
shoulders, lower back, knees and ankles. With regards to her 
recent dx of SLE, she reports not taking the Azathioprine due to 
lack of insurance coverage. ROS is also notable for progressive 
dysphagia, intially intermittent with a sensation of pain and 
food getting stuck. Now she says it is constant and occurs 
without eating. She reports no weight loss however. No fevers, 
chills, + sweats, + abdominal pain today ?___ to drinking lots 
of milk. No nausea/vomiting/diarrhea. No rash. +alopecia, no 
oral ulcers. Other ROS negative in detail.  

In the ED, VS: 97.7 88 143/88 16. Given 1L NS, morphine, 35 mg 
prednisone, Tegretol 200 mg, zofran 4 mg, Vicodine 2 tabs,  
UCG negative in ED. Evaluated by Neurology and Rheumatology. 
Tegretol level subtherapeutic. ESR/CRP low/nl. CXR and hip 
films. MRI of the ___ and head done in the ED, wet read 
negative.  
 
Upon initial evaluation the morning after admission, she 
confirms the story as stated above. States pain began 
approximately 1 week prior to admission and has always been 
worse in her left hip vs right. Also along inguinal tract, 
denies lyphadenopathy. No new sexual contacts or changed vaginal 
discharge. Has difficulty recalling seizures or 'black outs' but 
states they were witnessed by hre children. Currently has 
headache and feels nauseated, but denies photophobia. Started 
vomiting after Percocet, denies knowledge of prior narcotic 
reaction. Confirms dysphagia with solids and liquids. Son once 
thought of peforming the Heimlich on her given the severity of 
her pain. Feels very sleepy currently secondary to antiemetic 
and pain medication. No other sick contacts and denies F/ch, 
dysuria or rash. Also c/o new onset L shoulder pain with mild 
weakness, denies trauma but agrees that she may have hurt it 
during prior LOC.  
 
Past Medical History:
- SLE dx ___ titer ___, speckled, 
anti-Sm+, anti-RNP+, anti-ds neg, normal C3,C4). ___ MRI 
brain shows abn signal in midbrain-pontine juntion centrally. 
Started on Steroids/Azathioprine on ___.  
- Seizures --LOC episodes concerning for seizure-followed by 
___ started on Tegretol ___. Had been on 
Depakote prior  
to this but stopped for rash.  
- migraines  
- depression  
- HTN  
- Amenorrhea  
- KERATOCONUS s/p R corneal transplant ___  

 
Social History:
___
Family History:
Multiple family members on paternal side with lupus, one 
maternal uncle with lupus. Reports 2 family members on paternal 
side with CVA < ___ y/o. One paternal aunt with SLE and seizures. 

 
Physical Exam:
VS: 98.4 126/72 102 16 97% RA  
Gen: well appearing, lying flat, NAD, somnolent but rousable  
Heent: OP clear, anicteric, PERRL, EOMI, no evidence of thrush  
Neck: supple, 2+ carotids  
Lungs: CTA b/l  
CV: ___ low pitched systolic murmur best heard at R/L USB 
radiating to carotids b/l R>L, tachycardia  
Abd: soft, NT, ND, BS+  
Ext: warm, no c/c/e, tender hips b/l, notably tender along 
inguinal canal but no lymphadenopathy, patient not manipulated 
at her request  
Neuro: A&O, CN II-XII intact, rest of exam deferred.  

 
Pertinent Results:
___ 01:50PM   WBC-21.0* RBC-3.94* HGB-11.7* HCT-37.3 MCV-95 
MCH-29.7 MCHC-31.4 RDW-13.4  NEUTS-72.0* ___ MONOS-6.0 
EOS-0.3 BASOS-0.1

___ 01:50PM   PLT COUNT-345

___ 01:50PM   GLUCOSE-74 UREA N-18 CREAT-0.5 SODIUM-137 
POTASSIUM-4.1 CHLORIDE-102 TOTAL CO2-30 ANION GAP-9

___ 01:50PM   ALBUMIN-3.8 CALCIUM-9.4 PHOSPHATE-3.3 
MAGNESIUM-2.1

___ 01:50PM   ALT(SGPT)-25 AST(SGOT)-25 LD(LDH)-196 ALK 
PHOS-51 TOT BILI-0.3

___ 01:50PM   SED RATE-11

___ 03:25PM URINE  BLOOD-NEG NITRITE-NEG PROTEIN-NEG 
GLUCOSE-NEG KETONE-NEG BILIRUBIN-NEG UROBILNGN-NEG PH-5.0 
LEUK-NEG COLOR-Yellow APPEAR-Clear SP ___

___ 04:50AM BLOOD WBC-11.9* RBC-3.52* Hgb-10.6* Hct-32.8* 
MCV-93 MCH-30.1 MCHC-32.3 RDW-13.6 Plt ___

___ 05:00AM BLOOD Glucose-78 UreaN-12 Creat-0.5 Na-139 
K-3.9 Cl-105 HCO3-28 AnGap-10

___ 04:50AM BLOOD TSH-1.2

___ 05:10AM BLOOD ___ Titer-1:640  PAT 
Cntromr-NEGATIVE
___ 01:50PM BLOOD CRP-0.5

___ 05:10AM BLOOD ALDOLASE-PND
___ 05:10AM BLOOD SCLERODERMA ANTIBODY-PND

MR HEAD W & ___ CONTRAST  ___ 8:49 ___
Slightly improved or stable abnormal signal at the midbrain- 
pontine junction. This is a nonspecific finding, could be 
consistent with slow venous flow, small vessel disease from 
vasculitis such as lupus. Other remote considerations include 
toxic or metabolic causes for myelinolysis or Wernicke's 
encephalomyopathy. Although there is no susceptibility artifact, 
vascular malformation would also be potential consideration.

MR ___ CONTRAST  ___ 8:49 ___
There is no evidence of abnormality within the spinal cord; 
there is no evidence of spinal canal stenosis or neural 
foraminal narrowing. After the administration of gadolinium 
contrast, there is no evidence of abnormal enhancement. The 
signal intensity in the bone marrow is slightly hypointense 
suggesting anemic changes.

BILAT HIPS (AP,LAT & AP PELVIS)  ___ 3:18 ___
Essentially unremarkable study, with no specific evidence of 
avascular necrosis, and no subluxation, fracture or significant 
degenerative change.

MR HIP ___ CONRAST LEFT  ___ 9:43 ___
1. No evidence of osteonecrosis or fracture. Minimal fluid in 
both hip joints, but no frank effusion.
2. Mild assymetry of pirifiormis muscles noted. In the absence 
of symptoms related to the sciatic nerve, this is unlikely to 
account for the acute symptoms.

CAROTID SERIES COMPLETE  ___ 2:51 ___
Duplex evaluation was performed of bilateral carotid arteries. 
There is no significant plaque seen bilaterally.  On the right, 
peak velocities are 78, 77, and 54 cm/sec in the ICA, CCA, and 
ECA respectively. This is consistent with no stenosis.  On the 
left, peak velocities are 85, 106, and 73 cm/sec in the ICA, 
CCA, and ECA respectively. This is consistent with no stenosis.  
There is antegrade vertebral flow bilaterally. Normal carotid 
study.

TTE (Complete) Done ___ at 2:42:03 ___      FINAL 
The left atrium is normal in size. Left ventricular wall 
thicknesses and cavity size are normal. There is mild global 
left ventricular hypokinesis (LVEF = 50 %). Right ventricular 
chamber size and free wall motion are normal. The aortic valve 
leaflets (3) appear structurally normal with good leaflet 
excursion and no aortic regurgitation. No masses or vegetations 
are seen on the aortic valve. The mitral valve leaflets are 
mildly thickened. No mass or vegetation is seen on the mitral 
valve. There is moderate focal nodular thickening of the mitral 
valve chordae. Mild (1+) mitral regurgitation is seen. Mild 
pulmonic regurgitation is seen. There is no pericardial 
effusion.  IMPRESSION: Low normal left ventricular systolic 
function. Mild mitral regurgitation. Focal thickening of the 
mitral chordae and thickening mitral valve leaflets. 

ESOPHAGUS  ___ 1:50 ___
Limited study. No evidence of achalasia or esophageal stricture. 
Findings consistent with esophageal dysmotility.

EGD ___
IMPRESSION: Medium hiatal hernia, Gastritis (biopsy), otherwise 
normal EGD to third part of the duodenum. Recommendations: 
Follow-up biopsies, increase PPI to 20mg BID (or 40mg daily), no 
explanation for symptoms on EGD. Recommend outpatient esophageal 
manometry. 
 
Brief Hospital Course:
A/P: ___ F with recent dx of SLE with cerebral involvement, 
worsening seizure activity, b/l hip pain.  
  
# Hip Pain. Progressive pain leading to inability to ambulate, 
initially differential included polyarthralgia of SLE vs. 
arthritis (unlikely) vs. avascular necrosis of the hip (b/l 
would be unlikely) vs. septic joint given recent steroid use and 
relatively immunocompromised state. Given lack of other systemic 
symptoms and not exquisitely tender on exam, septic joint was 
thought to be less likely. Also complained of bilateral inguinal 
component concerning for intraabdominal process. No history of 
abnormal gynecologic findings or new sexual partners. MRI of 
hips was obtained as above to assess for avascular necrosis and 
was negative.  SLE was treated as described below.  Pain control 
was attained by scheduled Tylenol and oxycodone PRN, which she 
tolerated well.  Additionally, Rheumatology was consulted and 
recommended checking anti-centromere ab, anti-SCL 70 Ab and 
aldolase levels, all of which were pending upon discharge.  By 
discharge, pain was much improved with pain management as above, 
most consistent with arthralgia vs myalgia secondary to systemic 
disease. 

# SLE. Rheumatology consulted upon admission. Multiple 
manifestations were noted upon presentation including 
arthralgias, alopecia and CNS manifestations.  Prednisone was 
continued and Azathioprine was started.  Per Rheumatology 
recommendations, anti-centromere ab, anti-SCL 70 Ab and aldolase 
levels, were checked and all of which were pending upon 
discharge.  Additionally recommended PFTs with DLCO, which was 
not performed as an inpatient, but deferred to outpatient 
therapy.  Upon discharge, patient was overall improved and had 
scheduled outpatient follow-up.

# Seizures and loss of consciousness. Neurology consulted upon 
admission.  MRI brain and ___ were obtained in the ED and 
did reveal significant change. Initial thoughts included SLE 
cerebritis. Tegretol level was checked and found to be 
subtherapeutic, likely secondary to nausea and lack of PO 
intake. Per their recommendations, her anti-epileptic 
medications was changed to Keppra given that it can be given 
IV/po which will be an issue given her recurrent nausea with 
migraines.  Echo and carotid ultrasound were also only revealed 
mild decreased EF.  While inpatient she was started on Keppra 
with planned up titration by 500 mg BID x 3 days, then 750 mg 
BID x 3 days, then 1000 mg BID as new outpatient regimen.  She 
was additionally bridged with Ativan 0.5 mg po BID x 3 days with 
taper as outpatient.  No evidence of seizure while inpatient, 
and patient was monitored on telemetry throughout.  Also, 
continued Amitriptyline HCl 100 mg PO HS for migraines.  
  
# Dysphagia.  Concerning for overlap with Scleroderma vs. 
relation to underlying SLE. Antibodies as above. Speech and 
swallow evaluation revealed esophageal dysphagia, but no 
evidence of an oropharyngeal component. Initial considerations 
included Schatzki's ring, achalasia, thrush or other dysmotility 
syndrome. No evidence of thrush on exam but was on chronic 
steroids.  Esophageal study revealed dysmotility.  EGD was only 
revealing for mild gastritis but no other structural 
abnormality.  Biopsies were taken and pending on discharge with 
plan to follow-up with GI.  Was also to get outpatient manometry 
study, which was scheduled prior to discharge.  Thrush was 
empirically treated with Clotrimazole.  And PPI was increased to 
BID per GI.  Dysphagia had improved prior to discharge, but 
continued to have intermittent globus sensation. 

# Decreased Ejection Fraction: Noted on Echo that was obtained 
to assess for loss of consciousness.  Given this, she was 
started on low dose Metoprolol which she tolerated well.

# Tachycardia: Patient was persistently tachycardic during her 
inpatient stay.  This was thought to be due to pain and nausea.  
She remained euvolemic after initial fluid given for nausea and 
vomiting.  TSH was checked and found to be within normal limits. 
 Stable upon discharge.

# HTN. Continued propranolol and HCTZ, Metoprolol was started 
while inpatient and she was given this medication upon 
discharge.  

# GERD. PPI BID as above.  
  
# Migraines. Continued Amitriptyline. Was also given one dose of 
Imitrex which did relieve her symptoms.  Discharge on 
Amitriptyline.  

# FULL CODE  
 
Medications on Admission:
- Alendronate 35 mg 1 tablet po qweek  
- Almotriptan 12.5 mg prn HA --not taking  
- Azathioprine 50 mg tablet daily (start ___ x 2 weeks and 
then 2 tablets per day--***NOT taking  
- Tegretol 200 mg TID  
- HCTZ 25 mg qAM  
- Zofran 8 mg prn  
- Prilosec 20 mg daily  
- Prednisone 40 mg qday  
- propranolol 160 mg daily  
- Prochlorperazine 10 mg bid prn HA/nausea  
- Amitriptyline 100 mg HS  
- Calcium + D  
- Tylenol ___ mg tab TID prn  
 
Discharge Medications:
1. Azathioprine 50 mg Tablet Sig: One (1) Tablet PO DAILY 
(Daily).
Disp:*30 Tablet(s)* Refills:*0*
2. Hydrochlorothiazide 12.5 mg Capsule Sig: One (1) Capsule PO 
DAILY (Daily).  
3. Amitriptyline 25 mg Tablet Sig: Four (4) Tablet PO HS (at 
bedtime).  
4. Prochlorperazine Maleate 10 mg Tablet Sig: One (1) Tablet PO 
Q6H (every 6 hours) as needed for nausea for 10 doses.
Disp:*10 Tablet(s)* Refills:*0*
5. Propranolol 80 mg Capsule,Sustained Action 24 hr Sig: Two (2) 
Capsule,Sustained Action 24 hr PO DAILY (Daily).  
6. CALCIUM 500+D 500 (1,250)-200 mg-unit Tablet Sig: One (1) 
Tablet PO twice a day: Can be purchased over the counter at your 
pharmacy .  
7. Lorazepam 0.5 mg Tablet Sig: One (1) Tablet PO As directed 
for 7 doses: Take one tablet twice daily for 2 days; then one 
tablet daily for 3 days; then stop.
Disp:*7 Tablet(s)* Refills:*0*
8. Prilosec 40 mg Capsule, Delayed Release(E.C.) Sig: One (1) 
Capsule, Delayed Release(E.C.) PO once a day.
Disp:*30 Capsule, Delayed Release(E.C.)(s)* Refills:*2*
9. Acetaminophen 325 mg Tablet Sig: Two (2) Tablet PO Q6H (every 
6 hours) as needed for fever or pain.  
10. Docusate Sodium 100 mg Capsule Sig: One (1) Capsule PO BID 
(2 times a day) as needed for constipation: This medication can 
be bought over the counter at the pharmacy to treat 
constipation.  
11. Senna 8.6 mg Tablet Sig: One (1) Tablet PO BID (2 times a 
day): This medication can be bought over the counter at the 
pharmacy to treat constipation.  
12. Prednisone 20 mg Tablet Sig: Two (2) Tablet PO once a day.  
13. Oxycodone 5 mg Tablet Sig: One (1) Tablet PO every six (6) 
hours as needed for pain for 20 doses: This medication causes 
constipation.  You should take a laxative while taking this 
medication.
Disp:*20 Tablet(s)* Refills:*0*
14. Metoprolol Tartrate 25 mg Tablet Sig: 0.5 Tablet PO BID (2 
times a day).
Disp:*30 Tablet(s)* Refills:*2*
15. Keppra 250 mg Tablet Sig: ___ Tablets PO twice a day: Take 
two tablets twice daily for 2 days; then three tablets twice 
daily for 3 days; then take 4 tablets twice daily as your 
continued dosing.
Disp:*240 Tablet(s)* Refills:*2*
16. Alendronate 35 mg Tablet Sig: One (1) Tablet PO once a week. 
 
 
Discharge Disposition:
Home
 
Discharge Diagnosis:
Primary: Systemic Lupus Erythematosus, seizure disorder, 
migraines
Secondary: swallowing difficulties (dysphagia), mild heart 
failure 

 
Discharge Condition:
Stable, decreased hip pain, no further seizures

 
Discharge Instructions:
You were admitted with hip pain, nausea, vomiting and headache.  
You were found to have joint pain that was not due to fracture 
or other structural process.  You were treated for this pain.  
Your nausea and vomiting were due to Percocet, and this has been 
discontinued.  You have also been evaluated for your seizures 
and swallowing difficulties.  Upon discharge, you should 
follow-up with our GI and Rhuematology specialists.

Please keep all outpatient appointments.

Your medications have been changed.  Do not take Tegretol or 
Percocet.  You also have new medications including:
1) Keppra - for seizures
2) Metoprolol - for your heart
3) Azathioprine - for your lupus
4) Oxycodone - for your pain

Seek medical advice if you notice fevers, chills, inability to 
walk, inability to take in enough fluid to be hydrated, overall 
worsening of your condition or for any other symptom which is 
concerning to you.
 
Followup Instructions:
___
<|eot_id|><|start_header_id|>assistant<|end_header_id|>

A female patient presented with worsening hip pain; on physical examination, BP 126/72, HR 102, RR 16 and Temp 98.4; admission labs showed WBC 21.0, RBC 3.94, Hgb 11.7, Hct 37.3, Plt 345, MCV 95, MCH 29.7, MCHC 31.4, RDW 13.4, and Glucose 74; the most diagnostically relevant finding was bilateral hip pain without radiologic evidence of avascular necrosis, fracture, subluxation, or significant degenerative change.<|eot_id|>
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:11,  3.74s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.05s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:11<00:03,  3.83s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.95s/it]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
wandb: Tracking run with wandb version 0.22.2
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /root/HFRL/SurvLLM/wandb/offline-run-20251111_140519-zsvif0ue
======== Log a first sample from the processed training set ========
masking area: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ...
  0%|          | 0/360 [00:00<?, ?it/s]Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.
  0%|          | 1/360 [00:06<40:00,  6.69s/it]  1%|          | 2/360 [00:12<36:45,  6.16s/it]  1%|          | 3/360 [00:19<38:21,  6.45s/it]  1%|          | 4/360 [00:24<36:33,  6.16s/it]  1%|â–         | 5/360 [00:30<35:55,  6.07s/it]  2%|â–         | 6/360 [00:32<27:01,  4.58s/it]  2%|â–         | 7/360 [00:38<29:45,  5.06s/it]  2%|â–         | 8/360 [00:44<31:39,  5.40s/it]  2%|â–Ž         | 9/360 [00:51<33:50,  5.78s/it]                                                 2%|â–Ž         | 9/360 [00:51<33:50,  5.78s/it]{'loss': 1.2303, 'grad_norm': 0.5467361807823181, 'learning_rate': 7.272727272727273e-06, 'entropy': 2.1613974306318493, 'num_tokens': 268781.0, 'mean_token_accuracy': 0.7513823509216309, 'epoch': 1.0}
  3%|â–Ž         | 10/360 [00:57<34:32,  5.92s/it]  3%|â–Ž         | 11/360 [01:03<35:04,  6.03s/it]  3%|â–Ž         | 12/360 [01:10<35:13,  6.07s/it]  4%|â–Ž         | 13/360 [01:16<35:44,  6.18s/it]  4%|â–         | 14/360 [01:22<35:32,  6.16s/it]  4%|â–         | 15/360 [01:28<34:56,  6.08s/it]  4%|â–         | 16/360 [01:34<34:31,  6.02s/it]  5%|â–         | 17/360 [01:36<26:56,  4.71s/it]  5%|â–Œ         | 18/360 [01:42<29:17,  5.14s/it]                                                  5%|â–Œ         | 18/360 [01:42<29:17,  5.14s/it]{'loss': 1.1443, 'grad_norm': 0.4995405673980713, 'learning_rate': 9.934508710457944e-06, 'entropy': 2.1721817917293973, 'num_tokens': 537562.0, 'mean_token_accuracy': 0.7592067784733243, 'epoch': 2.0}
  5%|â–Œ         | 19/360 [01:48<31:12,  5.49s/it]  6%|â–Œ         | 20/360 [01:55<32:58,  5.82s/it]  6%|â–Œ         | 21/360 [02:01<33:36,  5.95s/it]  6%|â–Œ         | 22/360 [02:07<33:35,  5.96s/it]  6%|â–‹         | 23/360 [02:13<33:41,  6.00s/it]  7%|â–‹         | 24/360 [02:15<26:20,  4.70s/it]  7%|â–‹         | 25/360 [02:21<28:56,  5.18s/it]  7%|â–‹         | 26/360 [02:27<31:01,  5.57s/it]  8%|â–Š         | 27/360 [02:34<32:27,  5.85s/it]                                                  8%|â–Š         | 27/360 [02:34<32:27,  5.85s/it]{'loss': 0.9809, 'grad_norm': 0.46409618854522705, 'learning_rate': 9.595361424438841e-06, 'entropy': 2.1984451876746283, 'num_tokens': 806343.0, 'mean_token_accuracy': 0.777856535381741, 'epoch': 3.0}
  8%|â–Š         | 28/360 [02:41<33:41,  6.09s/it]  8%|â–Š         | 29/360 [02:47<33:33,  6.08s/it]  8%|â–Š         | 30/360 [02:53<34:04,  6.20s/it]  9%|â–Š         | 31/360 [03:00<34:43,  6.33s/it]  9%|â–‰         | 32/360 [03:01<26:59,  4.94s/it]  9%|â–‰         | 33/360 [03:08<28:52,  5.30s/it]  9%|â–‰         | 34/360 [03:14<30:51,  5.68s/it] 10%|â–‰         | 35/360 [03:21<32:04,  5.92s/it] 10%|â–ˆ         | 36/360 [03:27<32:29,  6.02s/it]                                                 10%|â–ˆ         | 36/360 [03:27<32:29,  6.02s/it]{'loss': 0.7947, 'grad_norm': 0.41583502292633057, 'learning_rate': 8.986093865669205e-06, 'entropy': 2.2342329025268555, 'num_tokens': 1075124.0, 'mean_token_accuracy': 0.803340368800693, 'epoch': 4.0}
 10%|â–ˆ         | 37/360 [03:33<33:26,  6.21s/it] 11%|â–ˆ         | 38/360 [03:39<32:56,  6.14s/it] 11%|â–ˆ         | 39/360 [03:46<33:13,  6.21s/it] 11%|â–ˆ         | 40/360 [03:53<34:06,  6.39s/it] 11%|â–ˆâ–        | 41/360 [03:59<33:31,  6.31s/it] 12%|â–ˆâ–        | 42/360 [04:00<26:05,  4.92s/it] 12%|â–ˆâ–        | 43/360 [04:07<28:27,  5.39s/it] 12%|â–ˆâ–        | 44/360 [04:13<29:44,  5.65s/it] 12%|â–ˆâ–Ž        | 45/360 [04:19<30:31,  5.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 12%|â–ˆâ–Ž        | 45/360 [04:21<30:31,  5.82s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 257.15it/s][A
                                              [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
                                                 12%|â–ˆâ–Ž        | 45/360 [05:48<30:31,  5.82s/it]{'eval_loss': 0.6250588297843933, 'eval_runtime': 1.9633, 'eval_samples_per_second': 0.509, 'eval_steps_per_second': 0.509, 'eval_entropy': 2.275681495666504, 'eval_num_tokens': 1343905.0, 'eval_mean_token_accuracy': 0.8538587689399719, 'epoch': 5.0}

Epoch 5 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 5 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_5_results.csv
{'loss': 0.6154, 'grad_norm': 0.4793882966041565, 'learning_rate': 8.142519434652782e-06, 'epoch': 5.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 13%|â–ˆâ–Ž        | 46/360 [05:54<2:50:09, 32.51s/it] 13%|â–ˆâ–Ž        | 47/360 [06:00<2:07:42, 24.48s/it] 13%|â–ˆâ–Ž        | 48/360 [06:06<1:38:19, 18.91s/it] 14%|â–ˆâ–Ž        | 49/360 [06:12<1:18:05, 15.07s/it] 14%|â–ˆâ–        | 50/360 [06:18<1:03:38, 12.32s/it] 14%|â–ˆâ–        | 51/360 [06:19<46:56,  9.12s/it]   14%|â–ˆâ–        | 52/360 [06:25<41:43,  8.13s/it] 15%|â–ˆâ–        | 53/360 [06:31<38:28,  7.52s/it] 15%|â–ˆâ–Œ        | 54/360 [06:38<36:50,  7.22s/it]                                                 15%|â–ˆâ–Œ        | 54/360 [06:38<36:50,  7.22s/it]{'loss': 0.4643, 'grad_norm': 0.41632080078125, 'learning_rate': 7.114224341122655e-06, 'entropy': 2.254786809285482, 'num_tokens': 1612686.0, 'mean_token_accuracy': 0.8641056219736735, 'epoch': 6.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 15%|â–ˆâ–Œ        | 55/360 [06:40<28:51,  5.68s/it] 16%|â–ˆâ–Œ        | 56/360 [06:46<29:13,  5.77s/it] 16%|â–ˆâ–Œ        | 57/360 [06:52<29:47,  5.90s/it] 16%|â–ˆâ–Œ        | 58/360 [06:58<29:39,  5.89s/it] 16%|â–ˆâ–‹        | 59/360 [07:04<29:57,  5.97s/it] 17%|â–ˆâ–‹        | 60/360 [07:10<30:04,  6.01s/it] 17%|â–ˆâ–‹        | 61/360 [07:17<30:43,  6.17s/it] 17%|â–ˆâ–‹        | 62/360 [07:23<30:22,  6.12s/it] 18%|â–ˆâ–Š        | 63/360 [07:30<31:24,  6.35s/it]                                                 18%|â–ˆâ–Š        | 63/360 [07:30<31:24,  6.35s/it]{'loss': 0.3618, 'grad_norm': 0.35308554768562317, 'learning_rate': 5.961652873455186e-06, 'entropy': 2.2337452835506864, 'num_tokens': 1881467.0, 'mean_token_accuracy': 0.922965751753913, 'epoch': 7.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 18%|â–ˆâ–Š        | 64/360 [07:36<31:52,  6.46s/it] 18%|â–ˆâ–Š        | 65/360 [07:42<31:06,  6.33s/it] 18%|â–ˆâ–Š        | 66/360 [07:49<31:19,  6.39s/it] 19%|â–ˆâ–Š        | 67/360 [07:51<24:23,  5.00s/it] 19%|â–ˆâ–‰        | 68/360 [07:57<26:03,  5.35s/it] 19%|â–ˆâ–‰        | 69/360 [08:03<27:10,  5.60s/it] 19%|â–ˆâ–‰        | 70/360 [08:10<28:18,  5.86s/it] 20%|â–ˆâ–‰        | 71/360 [08:16<28:43,  5.96s/it] 20%|â–ˆâ–ˆ        | 72/360 [08:23<30:03,  6.26s/it]                                                 20%|â–ˆâ–ˆ        | 72/360 [08:23<30:03,  6.26s/it]{'loss': 0.2989, 'grad_norm': 0.31999272108078003, 'learning_rate': 4.752554418576744e-06, 'entropy': 2.2180840969085693, 'num_tokens': 2150248.0, 'mean_token_accuracy': 0.9357730547587076, 'epoch': 8.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 20%|â–ˆâ–ˆ        | 73/360 [08:29<30:28,  6.37s/it] 21%|â–ˆâ–ˆ        | 74/360 [08:36<30:44,  6.45s/it] 21%|â–ˆâ–ˆ        | 75/360 [08:38<23:52,  5.03s/it] 21%|â–ˆâ–ˆ        | 76/360 [08:44<26:05,  5.51s/it] 21%|â–ˆâ–ˆâ–       | 77/360 [08:51<27:23,  5.81s/it] 22%|â–ˆâ–ˆâ–       | 78/360 [08:57<27:53,  5.94s/it] 22%|â–ˆâ–ˆâ–       | 79/360 [09:03<28:11,  6.02s/it] 22%|â–ˆâ–ˆâ–       | 80/360 [09:09<28:10,  6.04s/it] 22%|â–ˆâ–ˆâ–Ž       | 81/360 [09:16<28:35,  6.15s/it]                                                 22%|â–ˆâ–ˆâ–Ž       | 81/360 [09:16<28:35,  6.15s/it]{'loss': 0.2639, 'grad_norm': 0.2993355393409729, 'learning_rate': 3.5580010803434267e-06, 'entropy': 2.2121693558163114, 'num_tokens': 2419029.0, 'mean_token_accuracy': 0.9393889109293619, 'epoch': 9.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 23%|â–ˆâ–ˆâ–Ž       | 82/360 [09:22<28:57,  6.25s/it] 23%|â–ˆâ–ˆâ–Ž       | 83/360 [09:29<29:12,  6.33s/it] 23%|â–ˆâ–ˆâ–Ž       | 84/360 [09:35<29:30,  6.41s/it] 24%|â–ˆâ–ˆâ–Ž       | 85/360 [09:42<29:01,  6.33s/it] 24%|â–ˆâ–ˆâ–       | 86/360 [09:48<29:37,  6.49s/it] 24%|â–ˆâ–ˆâ–       | 87/360 [09:55<29:10,  6.41s/it] 24%|â–ˆâ–ˆâ–       | 88/360 [09:56<22:42,  5.01s/it] 25%|â–ˆâ–ˆâ–       | 89/360 [10:03<24:30,  5.43s/it] 25%|â–ˆâ–ˆâ–Œ       | 90/360 [10:09<25:51,  5.75s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/1 [00:00<?, ?it/s][A                                                
                                     [A 25%|â–ˆâ–ˆâ–Œ       | 90/360 [10:11<25:51,  5.75s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 102.90it/s][A
                                              [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                                 25%|â–ˆâ–ˆâ–Œ       | 90/360 [11:38<25:51,  5.75s/it]{'eval_loss': 0.3258230984210968, 'eval_runtime': 2.0642, 'eval_samples_per_second': 0.484, 'eval_steps_per_second': 0.484, 'eval_entropy': 2.236834764480591, 'eval_num_tokens': 2687810.0, 'eval_mean_token_accuracy': 0.9293924570083618, 'epoch': 10.0}

Epoch 10 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 10 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_10_results.csv
{'loss': 0.2434, 'grad_norm': 0.2641061246395111, 'learning_rate': 2.4482099850562496e-06, 'epoch': 10.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 25%|â–ˆâ–ˆâ–Œ       | 91/360 [11:44<2:25:41, 32.49s/it] 26%|â–ˆâ–ˆâ–Œ       | 92/360 [11:50<1:49:28, 24.51s/it] 26%|â–ˆâ–ˆâ–Œ       | 93/360 [11:56<1:24:46, 19.05s/it] 26%|â–ˆâ–ˆâ–Œ       | 94/360 [12:02<1:06:49, 15.07s/it] 26%|â–ˆâ–ˆâ–‹       | 95/360 [12:08<54:24, 12.32s/it]   27%|â–ˆâ–ˆâ–‹       | 96/360 [12:14<45:37, 10.37s/it] 27%|â–ˆâ–ˆâ–‹       | 97/360 [12:20<39:54,  9.11s/it] 27%|â–ˆâ–ˆâ–‹       | 98/360 [12:22<29:59,  6.87s/it] 28%|â–ˆâ–ˆâ–Š       | 99/360 [12:28<29:13,  6.72s/it]                                                 28%|â–ˆâ–ˆâ–Š       | 99/360 [12:28<29:13,  6.72s/it]{'loss': 0.2331, 'grad_norm': 0.2530185580253601, 'learning_rate': 1.4884158434739403e-06, 'entropy': 2.2087837987475925, 'num_tokens': 2956591.0, 'mean_token_accuracy': 0.9421407712830437, 'epoch': 11.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 28%|â–ˆâ–ˆâ–Š       | 100/360 [12:35<28:53,  6.67s/it] 28%|â–ˆâ–ˆâ–Š       | 101/360 [12:40<27:44,  6.43s/it] 28%|â–ˆâ–ˆâ–Š       | 102/360 [12:47<27:30,  6.40s/it] 29%|â–ˆâ–ˆâ–Š       | 103/360 [12:53<27:30,  6.42s/it] 29%|â–ˆâ–ˆâ–‰       | 104/360 [12:59<26:41,  6.26s/it] 29%|â–ˆâ–ˆâ–‰       | 105/360 [13:05<26:30,  6.24s/it] 29%|â–ˆâ–ˆâ–‰       | 106/360 [13:07<20:35,  4.86s/it] 30%|â–ˆâ–ˆâ–‰       | 107/360 [13:13<22:01,  5.22s/it] 30%|â–ˆâ–ˆâ–ˆ       | 108/360 [13:19<23:09,  5.51s/it]                                                  30%|â–ˆâ–ˆâ–ˆ       | 108/360 [13:19<23:09,  5.51s/it]{'loss': 0.228, 'grad_norm': 0.2318338304758072, 'learning_rate': 7.35036384554621e-07, 'entropy': 2.206082344055176, 'num_tokens': 3225372.0, 'mean_token_accuracy': 0.9422595898310343, 'epoch': 12.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 30%|â–ˆâ–ˆâ–ˆ       | 109/360 [13:26<24:11,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆ       | 110/360 [13:32<24:19,  5.84s/it] 31%|â–ˆâ–ˆâ–ˆ       | 111/360 [13:38<25:14,  6.08s/it] 31%|â–ˆâ–ˆâ–ˆ       | 112/360 [13:45<25:34,  6.19s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 113/360 [13:46<19:56,  4.84s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 114/360 [13:53<22:01,  5.37s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 115/360 [13:59<22:51,  5.60s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 116/360 [14:05<23:35,  5.80s/it] 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 117/360 [14:12<24:19,  6.01s/it]                                                  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 117/360 [14:12<24:19,  6.01s/it]{'loss': 0.2255, 'grad_norm': 0.2381431758403778, 'learning_rate': 2.3235606084334285e-07, 'entropy': 2.205061435699463, 'num_tokens': 3494153.0, 'mean_token_accuracy': 0.9428278472688463, 'epoch': 13.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 118/360 [14:19<25:21,  6.29s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 119/360 [14:25<25:26,  6.33s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 120/360 [14:27<19:50,  4.96s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 121/360 [14:33<21:17,  5.35s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 122/360 [14:39<22:06,  5.58s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 123/360 [14:46<22:46,  5.77s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 124/360 [14:53<24:15,  6.17s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 125/360 [14:59<24:31,  6.26s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 126/360 [15:06<24:41,  6.33s/it]                                                  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 126/360 [15:06<24:41,  6.33s/it]{'loss': 0.2249, 'grad_norm': 0.25181514024734497, 'learning_rate': 9.92296085934541e-09, 'entropy': 2.2046503755781384, 'num_tokens': 3762934.0, 'mean_token_accuracy': 0.9430342184172736, 'epoch': 14.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 127/360 [15:12<24:45,  6.37s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 128/360 [15:19<24:50,  6.43s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 129/360 [15:25<25:00,  6.50s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 130/360 [15:32<24:35,  6.42s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 131/360 [15:33<19:05,  5.00s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 132/360 [15:40<20:22,  5.36s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 133/360 [15:46<21:29,  5.68s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 134/360 [15:53<22:29,  5.97s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 135/360 [15:59<23:13,  6.19s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 38%|â–ˆâ–ˆâ–ˆâ–Š      | 135/360 [16:01<23:13,  6.19s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 162.77it/s][A
                                              [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                                  38%|â–ˆâ–ˆâ–ˆâ–Š      | 135/360 [17:25<23:13,  6.19s/it]{'eval_loss': 0.3058282732963562, 'eval_runtime': 2.0452, 'eval_samples_per_second': 0.489, 'eval_steps_per_second': 0.489, 'eval_entropy': 2.22473406791687, 'eval_num_tokens': 4031715.0, 'eval_mean_token_accuracy': 0.9269294142723083, 'epoch': 15.0}

Epoch 15 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 15 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_15_results.csv
{'loss': 0.2188, 'grad_norm': 0.25323957204818726, 'learning_rate': 9.919188059235483e-06, 'epoch': 15.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 136/360 [17:31<1:59:18, 31.96s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 137/360 [17:38<1:30:09, 24.26s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 138/360 [17:44<1:09:30, 18.79s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 139/360 [17:50<55:09, 14.98s/it]   39%|â–ˆâ–ˆâ–ˆâ–‰      | 140/360 [17:51<40:14, 10.97s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 141/360 [17:57<34:33,  9.47s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 142/360 [18:04<30:52,  8.50s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 143/360 [18:09<27:50,  7.70s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144/360 [18:15<25:57,  7.21s/it]                                                  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144/360 [18:15<25:57,  7.21s/it]{'loss': 0.1987, 'grad_norm': 0.24404478073120117, 'learning_rate': 9.559143929703724e-06, 'entropy': 2.1971935033798218, 'num_tokens': 4300496.0, 'mean_token_accuracy': 0.9450313183996413, 'epoch': 16.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 145/360 [18:22<25:02,  6.99s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 146/360 [18:28<24:10,  6.78s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 147/360 [18:34<23:07,  6.51s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 148/360 [18:40<22:41,  6.42s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 149/360 [18:46<22:01,  6.26s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/360 [18:53<22:16,  6.36s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/360 [18:59<22:00,  6.32s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/360 [19:05<21:42,  6.26s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153/360 [19:07<16:58,  4.92s/it]                                                  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153/360 [19:07<16:58,  4.92s/it]{'loss': 0.1837, 'grad_norm': 0.24700140953063965, 'learning_rate': 8.931108430602834e-06, 'entropy': 2.1884213818444147, 'num_tokens': 4569277.0, 'mean_token_accuracy': 0.9492679105864631, 'epoch': 17.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 154/360 [19:13<18:32,  5.40s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 155/360 [19:20<19:50,  5.81s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 156/360 [19:27<20:22,  5.99s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 157/360 [19:28<15:54,  4.70s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 158/360 [19:35<17:26,  5.18s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 159/360 [19:41<18:34,  5.54s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 160/360 [19:47<19:11,  5.76s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 161/360 [19:54<19:57,  6.02s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 162/360 [20:00<19:59,  6.06s/it]                                                  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 162/360 [20:00<19:59,  6.06s/it]{'loss': 0.1745, 'grad_norm': 0.17809700965881348, 'learning_rate': 8.071998162096615e-06, 'entropy': 2.1875451140933566, 'num_tokens': 4838058.0, 'mean_token_accuracy': 0.9502274526490105, 'epoch': 18.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 163/360 [20:07<20:25,  6.22s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 164/360 [20:13<20:34,  6.30s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 165/360 [20:19<20:21,  6.26s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 166/360 [20:26<20:19,  6.28s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 167/360 [20:32<20:27,  6.36s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 168/360 [20:34<15:54,  4.97s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 169/360 [20:40<17:04,  5.36s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 170/360 [20:47<18:20,  5.79s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 171/360 [20:53<18:34,  5.90s/it]                                                  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 171/360 [20:53<18:34,  5.90s/it]{'loss': 0.1658, 'grad_norm': 0.18596915900707245, 'learning_rate': 7.032312547106952e-06, 'entropy': 2.190967559814453, 'num_tokens': 5106839.0, 'mean_token_accuracy': 0.9529084298345778, 'epoch': 19.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 172/360 [21:00<19:06,  6.10s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 173/360 [21:06<19:09,  6.15s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 174/360 [21:13<19:26,  6.27s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 175/360 [21:19<19:27,  6.31s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 176/360 [21:25<19:12,  6.26s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 177/360 [21:32<19:31,  6.40s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 178/360 [21:34<15:10,  5.00s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 179/360 [21:40<16:08,  5.35s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/360 [21:46<17:17,  5.76s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/360 [21:48<17:17,  5.76s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 129.87it/s][A
                                              [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                                  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 180/360 [23:14<17:17,  5.76s/it]{'eval_loss': 0.2805784046649933, 'eval_runtime': 2.0322, 'eval_samples_per_second': 0.492, 'eval_steps_per_second': 0.492, 'eval_entropy': 2.224409818649292, 'eval_num_tokens': 5375620.0, 'eval_mean_token_accuracy': 0.9318554997444153, 'epoch': 20.0}

Epoch 20 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 20 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_20_results.csv
{'loss': 0.1593, 'grad_norm': 0.23086756467819214, 'learning_rate': 5.873165421077186e-06, 'epoch': 20.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 181/360 [23:20<1:35:48, 32.11s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 182/360 [23:26<1:12:10, 24.33s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 183/360 [23:32<55:20, 18.76s/it]   51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 184/360 [23:34<39:57, 13.62s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 185/360 [23:40<33:09, 11.37s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 186/360 [23:46<28:36,  9.87s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 187/360 [23:52<25:02,  8.68s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188/360 [23:58<22:34,  7.87s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 189/360 [24:04<20:46,  7.29s/it]                                                  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 189/360 [24:04<20:46,  7.29s/it]{'loss': 0.1539, 'grad_norm': 0.2074027806520462, 'learning_rate': 4.662692695184186e-06, 'entropy': 2.1902605957455106, 'num_tokens': 5644401.0, 'mean_token_accuracy': 0.9550811979505751, 'epoch': 21.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 190/360 [24:10<19:49,  6.99s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 191/360 [24:16<18:53,  6.70s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 192/360 [24:23<18:26,  6.58s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 193/360 [24:29<18:13,  6.55s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 194/360 [24:35<17:46,  6.42s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 195/360 [24:41<17:30,  6.36s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 196/360 [24:43<13:32,  4.95s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 197/360 [24:49<14:17,  5.26s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 198/360 [24:55<14:52,  5.51s/it]                                                  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 198/360 [24:55<14:52,  5.51s/it]{'loss': 0.15, 'grad_norm': 0.20306171476840973, 'learning_rate': 3.4720472544080895e-06, 'entropy': 2.188060681025187, 'num_tokens': 5913182.0, 'mean_token_accuracy': 0.9562730126910739, 'epoch': 22.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 199/360 [24:57<12:00,  4.48s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 200/360 [25:03<13:06,  4.91s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 201/360 [25:10<14:28,  5.46s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 202/360 [25:16<15:13,  5.78s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 203/360 [25:23<15:32,  5.94s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 204/360 [25:29<15:33,  5.98s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 205/360 [25:35<15:34,  6.03s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 206/360 [25:41<15:50,  6.17s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 207/360 [25:48<15:51,  6.22s/it]                                                  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 207/360 [25:48<15:51,  6.22s/it]{'loss': 0.1465, 'grad_norm': 0.19120655953884125, 'learning_rate': 2.371216514646428e-06, 'entropy': 2.1883024639553494, 'num_tokens': 6181963.0, 'mean_token_accuracy': 0.9575030538770888, 'epoch': 23.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 208/360 [25:54<15:59,  6.31s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 209/360 [26:01<16:07,  6.41s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 210/360 [26:07<16:04,  6.43s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 211/360 [26:13<15:41,  6.32s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 212/360 [26:20<15:31,  6.30s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 213/360 [26:21<12:05,  4.94s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 214/360 [26:28<13:14,  5.44s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 215/360 [26:34<13:45,  5.69s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 216/360 [26:41<14:15,  5.94s/it]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 216/360 [26:41<14:15,  5.94s/it]{'loss': 0.1444, 'grad_norm': 0.22278417646884918, 'learning_rate': 1.4249084873507412e-06, 'entropy': 2.187797784805298, 'num_tokens': 6450744.0, 'mean_token_accuracy': 0.9584265814887153, 'epoch': 24.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 217/360 [26:47<14:28,  6.07s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 218/360 [26:54<14:32,  6.15s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 219/360 [26:55<11:19,  4.82s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 220/360 [27:02<12:13,  5.24s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 221/360 [27:08<12:58,  5.60s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 222/360 [27:14<13:11,  5.74s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223/360 [27:21<13:48,  6.05s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224/360 [27:27<14:06,  6.22s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 225/360 [27:34<14:06,  6.27s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 225/360 [27:36<14:06,  6.27s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 103.52it/s][A
                                              [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                                  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 225/360 [28:59<14:06,  6.27s/it]{'eval_loss': 0.27667632699012756, 'eval_runtime': 2.0404, 'eval_samples_per_second': 0.49, 'eval_steps_per_second': 0.49, 'eval_entropy': 2.223004102706909, 'eval_num_tokens': 6719525.0, 'eval_mean_token_accuracy': 0.9326764941215515, 'epoch': 25.0}

Epoch 25 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 25 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_25_results.csv
{'loss': 0.143, 'grad_norm': 0.1883668750524521, 'learning_rate': 6.887481732184137e-07, 'epoch': 25.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 226/360 [29:06<1:11:16, 31.92s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 227/360 [29:11<53:20, 24.06s/it]   63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 228/360 [29:17<41:02, 18.66s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 229/360 [29:19<29:33, 13.54s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 230/360 [29:25<24:20, 11.23s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 231/360 [29:31<20:55,  9.73s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 232/360 [29:37<18:15,  8.56s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 233/360 [29:43<16:32,  7.82s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 234/360 [29:49<15:36,  7.43s/it]                                                  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 234/360 [29:49<15:36,  7.43s/it]{'loss': 0.1424, 'grad_norm': 0.26796355843544006, 'learning_rate': 2.0600786499752668e-07, 'entropy': 2.1879673401514688, 'num_tokens': 6988306.0, 'mean_token_accuracy': 0.9588286810451083, 'epoch': 26.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 235/360 [29:56<14:42,  7.06s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 236/360 [30:02<14:00,  6.78s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 237/360 [30:08<13:31,  6.60s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 238/360 [30:14<13:14,  6.51s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 239/360 [30:20<12:49,  6.36s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 240/360 [30:26<12:31,  6.26s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 241/360 [30:33<12:34,  6.34s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 242/360 [30:39<12:15,  6.23s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 243/360 [30:41<09:32,  4.90s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 243/360 [30:41<09:32,  4.90s/it]{'loss': 0.1422, 'grad_norm': 0.2305016815662384, 'learning_rate': 5.0635557124362185e-09, 'entropy': 2.1877223120795355, 'num_tokens': 7257087.0, 'mean_token_accuracy': 0.9594692124260796, 'epoch': 27.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 244/360 [30:47<10:20,  5.35s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 245/360 [30:53<10:41,  5.58s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 246/360 [31:00<11:20,  5.97s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 247/360 [31:07<11:36,  6.16s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 248/360 [31:13<11:29,  6.16s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 249/360 [31:14<08:56,  4.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 250/360 [31:21<09:44,  5.32s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 251/360 [31:28<10:22,  5.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 252/360 [31:34<10:42,  5.95s/it]                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 252/360 [31:34<10:42,  5.95s/it]{'loss': 0.1404, 'grad_norm': 0.23682041466236115, 'learning_rate': 9.902273032637558e-06, 'entropy': 2.1861582862006292, 'num_tokens': 7525868.0, 'mean_token_accuracy': 0.9595020876990424, 'epoch': 28.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 253/360 [31:40<10:51,  6.09s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 254/360 [31:47<10:47,  6.11s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 255/360 [31:53<10:56,  6.25s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 256/360 [32:00<10:55,  6.31s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 257/360 [32:06<10:47,  6.28s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258/360 [32:08<08:21,  4.91s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259/360 [32:14<09:08,  5.43s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260/360 [32:21<09:40,  5.80s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 261/360 [32:27<09:56,  6.02s/it]                                                  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 261/360 [32:27<09:56,  6.02s/it]{'loss': 0.1324, 'grad_norm': 0.21799832582473755, 'learning_rate': 9.52144875476464e-06, 'entropy': 2.1842676798502603, 'num_tokens': 7794649.0, 'mean_token_accuracy': 0.9620954857932197, 'epoch': 29.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 262/360 [32:34<10:16,  6.29s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 263/360 [32:41<10:08,  6.27s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 264/360 [32:47<10:10,  6.36s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 265/360 [32:53<10:00,  6.32s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 266/360 [33:00<10:09,  6.48s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 267/360 [33:07<09:59,  6.45s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 268/360 [33:08<07:42,  5.03s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 269/360 [33:15<08:22,  5.52s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/360 [33:21<08:35,  5.72s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/360 [33:23<08:35,  5.72s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 124.97it/s][A
                                              [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                                  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 270/360 [34:42<08:35,  5.72s/it]{'eval_loss': 0.27655261754989624, 'eval_runtime': 2.0404, 'eval_samples_per_second': 0.49, 'eval_steps_per_second': 0.49, 'eval_entropy': 2.214893341064453, 'eval_num_tokens': 8063430.0, 'eval_mean_token_accuracy': 0.9334975481033325, 'epoch': 30.0}

Epoch 30 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 30 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_30_results.csv
{'loss': 0.124, 'grad_norm': 0.2544482946395874, 'learning_rate': 8.874848870131098e-06, 'epoch': 30.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 271/360 [34:48<44:29, 30.00s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 272/360 [34:54<33:22, 22.75s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 273/360 [35:00<25:44, 17.75s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 274/360 [35:06<20:21, 14.20s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 275/360 [35:07<14:46, 10.43s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 276/360 [35:13<12:39,  9.04s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 277/360 [35:19<11:20,  8.20s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 278/360 [35:25<10:14,  7.50s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 279/360 [35:32<09:43,  7.21s/it]                                                  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 279/360 [35:32<09:43,  7.21s/it]{'loss': 0.117, 'grad_norm': 0.23724214732646942, 'learning_rate': 8.000481213391422e-06, 'entropy': 2.1776974201202393, 'num_tokens': 8332211.0, 'mean_token_accuracy': 0.9660359289911058, 'epoch': 31.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 280/360 [35:38<09:12,  6.91s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 281/360 [35:44<08:46,  6.67s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 282/360 [35:46<06:42,  5.16s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 283/360 [35:52<06:57,  5.42s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 284/360 [35:58<07:10,  5.66s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 285/360 [36:04<07:19,  5.85s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 286/360 [36:10<07:15,  5.89s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 287/360 [36:16<07:15,  5.97s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 288/360 [36:23<07:25,  6.19s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 288/360 [36:23<07:25,  6.19s/it]{'loss': 0.11, 'grad_norm': 0.21598902344703674, 'learning_rate': 6.949742053110412e-06, 'entropy': 2.1770055294036865, 'num_tokens': 8600992.0, 'mean_token_accuracy': 0.9692489239904616, 'epoch': 32.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 289/360 [36:25<05:52,  4.96s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 290/360 [36:31<06:11,  5.31s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 291/360 [36:38<06:34,  5.72s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 292/360 [36:45<06:46,  5.97s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293/360 [36:51<06:43,  6.02s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294/360 [36:57<06:47,  6.17s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295/360 [37:03<06:39,  6.15s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296/360 [37:10<06:34,  6.17s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 297/360 [37:16<06:38,  6.33s/it]                                                  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 297/360 [37:16<06:38,  6.33s/it]{'loss': 0.1041, 'grad_norm': 0.2716537415981293, 'learning_rate': 5.784394963977813e-06, 'entropy': 2.17342127694024, 'num_tokens': 8869773.0, 'mean_token_accuracy': 0.9707943068610297, 'epoch': 33.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 298/360 [37:18<05:13,  5.06s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 299/360 [37:25<05:36,  5.51s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 300/360 [37:31<05:43,  5.73s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 301/360 [37:38<05:53,  5.99s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 302/360 [37:44<05:50,  6.05s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 303/360 [37:50<05:51,  6.16s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 304/360 [37:56<05:43,  6.13s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 305/360 [38:03<05:38,  6.15s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 306/360 [38:09<05:37,  6.24s/it]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 306/360 [38:09<05:37,  6.24s/it]{'loss': 0.0978, 'grad_norm': 0.22536034882068634, 'learning_rate': 4.572940297650749e-06, 'entropy': 2.171843793657091, 'num_tokens': 9138554.0, 'mean_token_accuracy': 0.9730289445983039, 'epoch': 34.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 307/360 [38:16<05:35,  6.33s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 308/360 [38:17<04:16,  4.94s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 309/360 [38:23<04:29,  5.28s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 310/360 [38:30<04:46,  5.72s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 311/360 [38:36<04:45,  5.82s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 312/360 [38:42<04:45,  5.95s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 313/360 [38:49<04:44,  6.05s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 314/360 [38:55<04:48,  6.26s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 315/360 [39:02<04:44,  6.33s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 315/360 [39:04<04:44,  6.33s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 99.39it/s][A
                                             [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                                  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 315/360 [40:23<04:44,  6.33s/it]{'eval_loss': 0.2872016429901123, 'eval_runtime': 2.034, 'eval_samples_per_second': 0.492, 'eval_steps_per_second': 0.492, 'eval_entropy': 2.2135777473449707, 'eval_num_tokens': 9407335.0, 'eval_mean_token_accuracy': 0.9326764941215515, 'epoch': 35.0}

Epoch 35 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 35 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_35_results.csv
{'loss': 0.0933, 'grad_norm': 0.27314287424087524, 'learning_rate': 3.3865886586211234e-06, 'epoch': 35.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 316/360 [40:30<22:35, 30.80s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 317/360 [40:31<15:47, 22.05s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 318/360 [40:37<12:00, 17.16s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 319/360 [40:43<09:26, 13.82s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 320/360 [40:50<07:43, 11.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 321/360 [40:55<06:25,  9.88s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 322/360 [41:01<05:30,  8.70s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 323/360 [41:08<04:54,  7.97s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 324/360 [41:14<04:24,  7.36s/it]                                                  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 324/360 [41:14<04:24,  7.36s/it]{'loss': 0.0896, 'grad_norm': 0.292812705039978, 'learning_rate': 2.2950750685150045e-06, 'entropy': 2.172336604860094, 'num_tokens': 9676116.0, 'mean_token_accuracy': 0.9748130242029825, 'epoch': 36.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 325/360 [41:20<04:05,  7.02s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 326/360 [41:26<03:52,  6.85s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 327/360 [41:32<03:37,  6.60s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 328/360 [41:39<03:28,  6.52s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329/360 [41:45<03:19,  6.44s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330/360 [41:51<03:12,  6.43s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331/360 [41:53<02:25,  5.01s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332/360 [41:59<02:28,  5.31s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 333/360 [42:05<02:30,  5.59s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 333/360 [42:05<02:30,  5.59s/it]{'loss': 0.0869, 'grad_norm': 0.24683773517608643, 'learning_rate': 1.362559866735142e-06, 'entropy': 2.1726753976609974, 'num_tokens': 9944897.0, 'mean_token_accuracy': 0.9761388831668429, 'epoch': 37.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 334/360 [42:12<02:31,  5.84s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 335/360 [42:18<02:32,  6.10s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 336/360 [42:24<02:26,  6.10s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 337/360 [42:26<01:50,  4.79s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 338/360 [42:33<01:57,  5.33s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 339/360 [42:39<01:57,  5.60s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 340/360 [42:46<01:57,  5.90s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 341/360 [42:52<01:55,  6.06s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 342/360 [42:59<01:51,  6.19s/it]                                                  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 342/360 [42:59<01:51,  6.19s/it]{'loss': 0.0851, 'grad_norm': 0.24217592179775238, 'learning_rate': 6.438572969032065e-07, 'entropy': 2.1729127830929227, 'num_tokens': 10213678.0, 'mean_token_accuracy': 0.9765715599060059, 'epoch': 38.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 343/360 [43:05<01:45,  6.24s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 344/360 [43:11<01:39,  6.22s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 345/360 [43:17<01:33,  6.22s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 346/360 [43:24<01:27,  6.26s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 347/360 [43:30<01:22,  6.34s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 348/360 [43:37<01:17,  6.46s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 349/360 [43:44<01:11,  6.50s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 350/360 [43:50<01:05,  6.56s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 351/360 [43:52<00:46,  5.14s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 351/360 [43:52<00:46,  5.14s/it]{'loss': 0.0843, 'grad_norm': 0.2178492546081543, 'learning_rate': 1.8121346683502238e-07, 'entropy': 2.1725518968370228, 'num_tokens': 10482459.0, 'mean_token_accuracy': 0.9770452645089891, 'epoch': 39.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 352/360 [43:59<00:44,  5.61s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 353/360 [44:05<00:41,  5.86s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 354/360 [44:12<00:36,  6.03s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 355/360 [44:18<00:31,  6.22s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 356/360 [44:25<00:24,  6.24s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 357/360 [44:26<00:14,  4.88s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 358/360 [44:33<00:10,  5.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 359/360 [44:39<00:05,  5.74s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [44:46<00:00,  5.95s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

  0%|          | 0/1 [00:00<?, ?it/s][A                                                 
                                     [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [44:48<00:00,  5.95s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 153.46it/s][A
                                              [Awandb: WARNING URL not available in offline run
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [46:00<00:00,  5.95s/it]                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [46:00<00:00,  5.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [46:00<00:00,  7.67s/it]
wandb: WARNING URL not available in offline run
{'eval_loss': 0.293597936630249, 'eval_runtime': 2.0066, 'eval_samples_per_second': 0.498, 'eval_steps_per_second': 0.498, 'eval_entropy': 2.2144877910614014, 'eval_num_tokens': 10751240.0, 'eval_mean_token_accuracy': 0.9351395964622498, 'epoch': 40.0}

Epoch 40 ì¢…ë£Œ. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¶”ë¡  ì‹œìž‘...
Epoch 40 ì¶”ë¡  ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: inference/lr1e-5/epoch_40_results.csv
{'loss': 0.0838, 'grad_norm': 0.2459353804588318, 'learning_rate': 1.8230770169841427e-09, 'epoch': 40.0}
{'train_runtime': 2761.7336, 'train_samples_per_second': 0.13, 'train_steps_per_second': 0.13, 'train_loss': 0.269537964132097, 'epoch': 40.0}
Processing Files (0 / 0)      : |          |  0.00B /  0.00B            
New Data Upload               : |          |  0.00B /  0.00B            [A

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:   5%|â–         | 33.4MB /  671MB            [A[A[A[A

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:   5%|â–         | 33.4MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :   7%|â–‹         | 50.6MB /  688MB,   ???B/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:   9%|â–Š         | 58.6MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  11%|â–ˆ         | 75.9MB /  688MB,  127MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  14%|â–ˆâ–Ž        | 92.3MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  16%|â–ˆâ–Œ        |  109MB /  688MB,  147MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  17%|â–ˆâ–‹        |  117MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  20%|â–ˆâ–‰        |  135MB /  688MB,  140MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  21%|â–ˆâ–ˆ        |  143MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  23%|â–ˆâ–ˆâ–Ž       |  160MB /  688MB,  137MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  26%|â–ˆâ–ˆâ–Œ       |  176MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  28%|â–ˆâ–ˆâ–Š       |  193MB /  688MB,  143MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  31%|â–ˆâ–ˆâ–ˆ       |  210MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  33%|â–ˆâ–ˆâ–ˆâ–Ž      |  227MB /  688MB,  147MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      |  243MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  38%|â–ˆâ–ˆâ–ˆâ–Š      |  260MB /  688MB,  150MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      |  277MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     |  294MB /  688MB,  152MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     |  310MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     |  328MB /  688MB,  154MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     |  336MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     |  353MB /  688MB,  151MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    |  369MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    |  386MB /  688MB,  153MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    |  394MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    |  411MB /  688MB,  150MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   |  428MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   |  445MB /  688MB,  152MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   |  453MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   |  470MB /  688MB,  150MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  |  486MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  |  504MB /  688MB,  151MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  |  512MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  |  529MB /  688MB,  149MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  |  545MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– |  562MB /  688MB,  151MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– |  570MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ |  588MB /  688MB,  149MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ |  604MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |  621MB /  688MB,  150MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž|  629MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–|  646MB /  688MB,  149MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š|  663MB /  671MB            [A[A[A[AProcessing Files (2 / 3)      :  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰|  680MB /  688MB,  150MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  671MB /  671MB            [A[A[A[AProcessing Files (3 / 3)      : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  688MB /  688MB,  145MB/s  

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  671MB /  671MB            [A[A[A[A

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  671MB /  671MB            [A[A[A[A

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  671MB /  671MB            [A[A[A[A

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  671MB /  671MB            [A[A[A[A

  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            [A[A


  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            [A[A[A



  ...adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  671MB /  671MB            [A[A[A[AProcessing Files (3 / 3)      : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  688MB /  688MB,  123MB/s  
New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  
  .../lr1e-5/training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.35kB / 6.35kB            
  ...lts/lr1e-5/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.2MB / 17.2MB            
  ...adapter_model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|  671MB /  671MB            
No files have been modified since last commit. Skipping to prevent empty commit.
/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Find logs at: /root/HFRL/SurvLLM/wandb/debug-cli.root.log
Syncing: https://wandb.ai/hollyriver-jbnu/huggingface/runs/dl1xy6vk ... done.
Working Time: 0:46:57
========== í•™ìŠµ ì¢…ë£Œ ==========
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /root/HFRL/SurvLLM/wandb/offline-run-20251111_140519-zsvif0ue[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20251111_140519-zsvif0ue/logs[0m
