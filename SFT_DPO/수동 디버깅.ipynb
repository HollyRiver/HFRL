{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678043ef",
   "metadata": {},
   "source": [
    "* flash-attention은 죄가 없음\n",
    "* fp32가 왜 나오는 걸까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f0483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass, field, fields    ## For TrlParser\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    set_seed\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig, TrlParser, setup_chat_format\n",
    "from peft import LoraConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1902b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\"json\", data_files = os.path.join(\"./data\", \"sft_train_dataset.json\"), split = \"train\")\n",
    "test_ds = load_dataset(\"json\", data_files = os.path.join(\"./data\", \"sft_test_dataset.json\"), split = \"train\")\n",
    "\n",
    "## 토크나이저 로드 및 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    use_fast = True,            ## Rust로 구현된 Fast Tokenizer 사용 (Qwen, RoPE, ChatGLM 등의 특이한 구조에서는 호환 안됨)\n",
    "    trust_remote_code = True)   ## 모델 코드 전체 다운로드 후 사용\n",
    "tokenizer.pad_token = tokenizer.eos_token       ## 패딩할 토큰 설정\n",
    "tokenizer.padding_side = \"left\"                 ## 디코더이므로 왼쪽을 패딩 (마지막 토큰을 보고 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffdd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + eos_token }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "            \"{% generation %}\"\n",
    "            \"{{ message['content'] +  eos_token }}\"\n",
    "            \"{% endgeneration %}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{%- if add_generation_prompt %}\"\n",
    "    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "    \"{%- endif %}\"\n",
    ")\n",
    "\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ffa308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Log a few random samples from the processed training set ========\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Read the passage below and choose the right answer to the following question (choices are increases or decreases ):  More UV rays increase skin cancer rates.   Mona lives in a country that experiences high levels of sunlight. If she moves to a less sunny country, what happens to her chance of getting skin cancer increases or decreases?\n",
      "decreases\n",
      "(Question)\n",
      "Read the passage below and choose the right answer to the following question (choices are expanding or contracting ):  When your brain senses that your body temperature is increasing, it sends messages to the blood vessels in the skin to increase in diameter.   Derek felt warm as he stood in the sun, and since his body was heating up, he knew that his blood vessels in his skin were expanding or contracting?\n",
      "(Answer)\n",
      "expanding\n",
      "Ques: Read the passage below and choose the right answer to the following question (choices are directly in front of the speaker or two miles away? ):  The farther the sound waves travel away from the source, the more spread out their energy becomes.   Would the sound from a loud speaker transmit more energy to the eardrum of a member of the audience if they were standing directly in front of the speaker or two miles away??\n",
      "Ans: directly in front of the speaker\n",
      "(Q).\n",
      "Read the passage below and choose the right answer to the following question (choices are less or more ):  As the rocks move deeper into the Earth, temperature and pressure increase.   Ulysses pilots his drill ship deep into the Earth's core, then back up. His ship is subjected to less or more temperature and pressure at the surface?\n",
      "(A).\n",
      "less\n",
      "Ques:Read the passage below and choose the right answer to the following question (choices are slower or not as slow ):  For a given medium, sound has a slower speed at lower temperatures.   Greta yells across the canyon in the cool morning and again in the hot afternoon. The sound likely traveled slower or not as slow in the morning?\n",
      "\n",
      "Ans:not as slow\n",
      "Read the passage below and choose the right answer to the following question (choices are more or less ):  Genotypes with greater fitness produce more offspring and pass more copies of their alleles to the next generation.   A genotype with good fitness will likely produce more or less offspring than a more fit one?\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure, I'm here to assist you! Here are the answers to the questions you've provided:\n",
      "\n",
      "1. If Mona moves to a less sunny country, her chance of getting skin cancer will decrease.\n",
      "2. When Derek felt warm, his body temperature was increasing, so his blood vessels in his skin were expanding.\n",
      "3. The sound from a loud speaker will transmit more energy to the eardrum of a member of the audience who is standing directly in front of the speaker, rather than two miles away.\n",
      "4. As the rocks move deeper into the Earth's core, the temperature and pressure will increase, so the ship will experience less temperature and pressure at the surface.\n",
      "5. The sound likely traveled slower in the cool morning than in the hot afternoon.\n",
      "6. A genotype with good fitness will likely produce more offspring than a genotype with more fitness.\n",
      "\n",
      "I hope these answers are helpful and accurate! Let me know if you have any other questions.<|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the task definition and input, reply with output. In this task, you are given a paragraph, a question, and a candidate incorrect answer to the question. Your goal is to judge whether the provided answer is a valid incorrect answer to a given question. An incorrect answer should not truthfully answer the given question. A good incorrect answer should be closely related to the content of the paragraph and/or the question so that the readers are forced to read the whole paragraph to infer its [in]correctness. Additionally, an incorrect answer should be of the same semantic type as the given correct answer (e.g., both can be names of locations). If you think the given incorrect answer is good(and incorrect), indicate it by responding \"Yes\". Otherwise, respond \"No\". There are only two types of responses possible:\"Yes\" and \"No\".\n",
      "\n",
      "Paragraph- Sent 1: `` The Octopus , '' a masked crime lord , is bent on crippling the nation with a wave of terror .\n",
      "Sent 2: He starts with the transportation system and then moves onto industry .\n",
      "Sent 3: He demands tribute from railroad magnates and other captains of industry .\n",
      "Sent 4: Richard Wentworth , an amateur criminologist who is friendly with the police , is secretly `` The Spider , '' a masked vigilante equally determined to wipe the Octopus and his gang off the face of the earth .\n",
      "Sent 5: Pleasant and smiling in civilian life , Wentworth is often ruthless as the Spider , slinging two guns against the public enemies who attack him .\n",
      "Sent 6: Wentworth also masquerades as affable underworld lowlife Blinky McQuade .\n",
      "Sent 7: Disguised as McQuade , Wentworth can infiltrate gangland at the hired-gun level and keep current on the gang 's plans .\n",
      "Sent 8: The only people who know Wentworth 's other identities are his assistants Jackson and Ram Singh , his butler Jenkins , and his fianc e Nita .\n",
      "Sent 9: The Octopus was a villain in a single issue pulp believed to have been written by Norvell Page who wrote most of The Spider pulp stories .\n",
      "Sent 10: He is garbed completely in white and is only ever seen sitting in a chair .\n",
      "Sent 11: Unlike the pulps , The Spider is garbed in a lightweight full length costume with web-like markings on it which resemble Spiderman 's costume .\n",
      "Sent 12: The serial follows the standard formula of fights , shoot-outs , Wentworth 's friends being kidnapped at various times and having to be rescued .\n",
      "Sent 13: Each chapter ends with The Spider or his friends in deep trouble , often about to be killed , but the effect is spoiled by a trailer for the next episode which shows them in full health and fighting the villains . \n",
      "Question: The Octopus is completely garbed in what color? \n",
      "Incorrect Answer: Red.\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "No.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "## 템플릿 적용사항 확인\n",
    "print(\"======== Log a few random samples from the processed training set ========\")\n",
    "for index in random.sample(range(len(train_ds)), 2):\n",
    "    print(tokenizer.apply_chat_template(train_ds[index][\"messages\"], tokenize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c7718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "## 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,                    ## 4비트 양자화\n",
    "    bnb_4bit_use_double_quant = True,       ## 추가 양자화로 성능 손실 없이 파라미터당 0.4bit 추가 절약\n",
    "    bnb_4bit_quant_type = \"nf4\",            ## 양자화 데이터 타입 지정: 4비트 기반 모델 훈련 시 사용\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 ## Llama-3.1-8B의 학습 자료형. 저장은 4비트지만, attention 연산은 해당 포맷으로 역양자화하여 처리\n",
    ")\n",
    "\n",
    "## 모델 로드 및 설정\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    device_map = \"cuda:0\",\n",
    "    use_cache = False,                          ## VRAM 캐시 미사용, 추론 속도 저하. gradienc_checkpointing과 동시 사용 불가\n",
    "    low_cpu_mem_usage = True,                   ## CPU RAM 사용량 적게 사용...\n",
    "    attn_implementation = \"flash_attention_2\",  ## flash_attention 연산 사용. sdpa가 더 빠르고 효율적일 수도 있음.\n",
    "    quantization_config = bnb_config,\n",
    "    dtype = torch.bfloat16                      ## 가중치 로드 데이터 타입. Llama-3.1-8B의 자료형으로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c883174",
   "metadata": {},
   "source": [
    "> 7800MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9ce230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"gate_proj\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01fb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    max_length = 1024,\n",
    "    output_dir = \"./results/test\",\n",
    "    report_to = \"wandb\",\n",
    "    assistant_only_loss = True,\n",
    "    learning_rate = 1e-4,\n",
    "    lr_scheduler_type = \"cosine_with_restarts\",\n",
    "    lr_scheduler_kwargs = {\"num_cycles\":3},\n",
    "    num_train_epochs = 6,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    do_eval = True,\n",
    "    eval_steps = 500,\n",
    "    eval_strategy = \"steps\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 100,\n",
    "    save_strategy = \"epoch\",\n",
    "    weight_decay = 0.01,\n",
    "    max_grad_norm = 0.5,\n",
    "    warmup_ratio = 0.06,\n",
    "    bf16 = True,\n",
    "    tf32 = True,\n",
    "    gradient_checkpointing = True,   \n",
    "    packing = True,\n",
    "    dataloader_num_workers = 4,\n",
    "    push_to_hub = True,\n",
    "    dataset_kwargs = {\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    processing_class = tokenizer,\n",
    "    peft_config = peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7699fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Log a first sample from the processed training set ========\n",
      "masking area: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ...\n"
     ]
    }
   ],
   "source": [
    "if training_args.assistant_only_loss:\n",
    "    print(\"======== Log a first sample from the processed training set ========\")\n",
    "    print(f\"masking area: {next(iter(trainer.train_dataset))[\"assistant_masks\"][:100]} ...\")\n",
    "\n",
    "## 학습이 중단된 경우 이어서 진행할 수 있도록 설정\n",
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ddd66e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 parameter: model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.0.input_layernorm.weight\n",
      "FP32 parameter: model.layers.0.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.1.input_layernorm.weight\n",
      "FP32 parameter: model.layers.1.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.2.input_layernorm.weight\n",
      "FP32 parameter: model.layers.2.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.3.input_layernorm.weight\n",
      "FP32 parameter: model.layers.3.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.4.input_layernorm.weight\n",
      "FP32 parameter: model.layers.4.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.5.input_layernorm.weight\n",
      "FP32 parameter: model.layers.5.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.6.input_layernorm.weight\n",
      "FP32 parameter: model.layers.6.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.7.input_layernorm.weight\n",
      "FP32 parameter: model.layers.7.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.8.input_layernorm.weight\n",
      "FP32 parameter: model.layers.8.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.9.input_layernorm.weight\n",
      "FP32 parameter: model.layers.9.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.10.input_layernorm.weight\n",
      "FP32 parameter: model.layers.10.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.11.input_layernorm.weight\n",
      "FP32 parameter: model.layers.11.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.12.input_layernorm.weight\n",
      "FP32 parameter: model.layers.12.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.13.input_layernorm.weight\n",
      "FP32 parameter: model.layers.13.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.14.input_layernorm.weight\n",
      "FP32 parameter: model.layers.14.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.15.input_layernorm.weight\n",
      "FP32 parameter: model.layers.15.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.16.input_layernorm.weight\n",
      "FP32 parameter: model.layers.16.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.17.input_layernorm.weight\n",
      "FP32 parameter: model.layers.17.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.18.input_layernorm.weight\n",
      "FP32 parameter: model.layers.18.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.19.input_layernorm.weight\n",
      "FP32 parameter: model.layers.19.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.20.input_layernorm.weight\n",
      "FP32 parameter: model.layers.20.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.21.input_layernorm.weight\n",
      "FP32 parameter: model.layers.21.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.22.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.22.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.22.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.22.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.22.input_layernorm.weight\n",
      "FP32 parameter: model.layers.22.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.23.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.23.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.23.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.23.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.23.input_layernorm.weight\n",
      "FP32 parameter: model.layers.23.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.24.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.24.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.24.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.24.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.24.input_layernorm.weight\n",
      "FP32 parameter: model.layers.24.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.25.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.25.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.25.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.25.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.25.input_layernorm.weight\n",
      "FP32 parameter: model.layers.25.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.26.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.26.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.26.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.26.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.26.input_layernorm.weight\n",
      "FP32 parameter: model.layers.26.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.27.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.27.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.27.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.27.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.27.input_layernorm.weight\n",
      "FP32 parameter: model.layers.27.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.28.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.28.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.28.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.28.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.28.input_layernorm.weight\n",
      "FP32 parameter: model.layers.28.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.29.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.29.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.29.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.29.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.29.input_layernorm.weight\n",
      "FP32 parameter: model.layers.29.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.30.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.30.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.30.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.30.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.30.input_layernorm.weight\n",
      "FP32 parameter: model.layers.30.post_attention_layernorm.weight\n",
      "FP32 parameter: model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.31.mlp.up_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.31.mlp.up_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.31.mlp.down_proj.lora_A.default.weight\n",
      "FP32 parameter: model.layers.31.mlp.down_proj.lora_B.default.weight\n",
      "FP32 parameter: model.layers.31.input_layernorm.weight\n",
      "FP32 parameter: model.layers.31.post_attention_layernorm.weight\n",
      "FP32 parameter: model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.dtype == torch.float32:\n",
    "        print(\"FP32 parameter:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6166a633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhollyriver\u001b[0m (\u001b[33mhollyriver-jbnu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/HFRL/SFT_DPO/wandb/run-20251118_015630-tta662q2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hollyriver-jbnu/huggingface/runs/tta662q2' target=\"_blank\">sleek-jazz-18</a></strong> to <a href='https://wandb.ai/hollyriver-jbnu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hollyriver-jbnu/huggingface' target=\"_blank\">https://wandb.ai/hollyriver-jbnu/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hollyriver-jbnu/huggingface/runs/tta662q2' target=\"_blank\">https://wandb.ai/hollyriver-jbnu/huggingface/runs/tta662q2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/LLM/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='4596' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/4596 : < :, Epoch 0.00/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/transformers/trainer.py:2316\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2313\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2314\u001b[39m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[32m   2315\u001b[39m     hf_hub_utils.disable_progress_bars()\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2322\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2323\u001b[39m     hf_hub_utils.enable_progress_bars()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1190\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1189\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x70603b1a51c0>> (for post_run_cell), with arguments args (<ExecutionResult object at 70603b1d9e80, execution_count=9 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 70603b1da060, raw_cell=\"trainer.train(resume_from_checkpoint = checkpoint)\" transformed_cell=\"trainer.train(resume_from_checkpoint = checkpoint)..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B113.198.65.146/root/HFRL/SFT_DPO/%EC%88%98%EB%8F%99%20%EB%94%94%EB%B2%84%EA%B9%85.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:603\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:820\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    819\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/LLM/lib/python3.12/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e95f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m model.gradient_checkpointing_enable()\n\u001b[32m      3\u001b[39m peft_config = LoraConfig(\n\u001b[32m      4\u001b[39m     r = \u001b[32m32\u001b[39m,\n\u001b[32m      5\u001b[39m     lora_alpha = \u001b[32m16\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     ]\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m trainer = SFTTrainer(\n\u001b[32m     19\u001b[39m     model = model,\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     args = \u001b[43mtraining_args\u001b[49m,\n\u001b[32m     21\u001b[39m     train_dataset = train_ds,\n\u001b[32m     22\u001b[39m     eval_dataset = test_ds,\n\u001b[32m     23\u001b[39m     processing_class = tokenizer,\n\u001b[32m     24\u001b[39m     peft_config = peft_config\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_args.assistant_only_loss:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m======== Log a first sample from the processed training set ========\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'training_args' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2880c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
