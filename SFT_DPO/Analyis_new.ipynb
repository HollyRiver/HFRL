{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb894f0",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import os\n",
    "\n",
    "from trl import DPOTrainer\n",
    "from huggingface_hub import login\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff1cd2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "* LLM을 통한 Feedback 자동 수집 자료 https://arxiv.org/abs/2310.01377\n",
    "* Implicit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78938f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "## 원시 데이터 로드\n",
    "ds = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b7447c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 28/28 [00:01<00:00, 15.83ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 19.12ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 28/28 [00:03<00:00,  8.52ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 10.96ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10251164"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_split = ds[\"train\"].train_test_split(test_size = 0.5, seed = 42)\n",
    "\n",
    "## For SFT\n",
    "sft_ds = ds_split[\"train\"]\n",
    "sft_ds = sft_ds.rename_column(\"chosen\", \"messages\").remove_columns([col for col in sft_ds.column_names if col != \"chosen\"]).train_test_split(test_size = 0.1, seed = 42)\n",
    "sft_ds[\"train\"].to_json(\"./data/sft_train_dataset.json\", orient = \"records\")\n",
    "sft_ds[\"test\"].to_json(\"./data/sft_test_dataset.json\", orient = \"records\")\n",
    "\n",
    "## Implicit Prompt -> Explicit Prompt\n",
    "dpo_ds = ds_split[\"test\"].map(\n",
    "    lambda sample: {\n",
    "        \"prompt\": [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n",
    "        \"chosen\": [content for content in sample[\"chosen\"] if content[\"role\"] == \"assistant\"],\n",
    "        \"rejected\": [content for content in sample[\"rejected\"] if content[\"role\"] == \"assistant\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "dpo_ds = dpo_ds.remove_columns([col for col in dpo_ds.column_names if col not in [\"prompt\", \"chosen\", \"rejected\"]]).train_test_split(test_size = 0.1, seed = 42)\n",
    "dpo_ds[\"train\"].to_json(\"./data/dpo_train_dataset.json\", orient = \"records\")\n",
    "dpo_ds[\"test\"].to_json(\"./data/dpo_test_dataset.json\", orient = \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935eda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard format\n",
    "## Explicit prompt (recommended)\n",
    "preference_example = {\"prompt\": \"The sky is\", \"chosen\": \" blue.\", \"rejected\": \" green.\"}\n",
    "# Implicit prompt\n",
    "preference_example = {\"chosen\": \"The sky is blue.\", \"rejected\": \"The sky is green.\"}\n",
    "\n",
    "# Conversational format\n",
    "## Explicit prompt (recommended)\n",
    "preference_example = {\"prompt\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"}],\n",
    "                      \"chosen\": [{\"role\": \"assistant\", \"content\": \"It is blue.\"}],\n",
    "                      \"rejected\": [{\"role\": \"assistant\", \"content\": \"It is green.\"}]}\n",
    "## Implicit prompt\n",
    "preference_example = {\"chosen\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"},\n",
    "                                 {\"role\": \"assistant\", \"content\": \"It is blue.\"}],\n",
    "                      \"rejected\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"},\n",
    "                                   {\"role\": \"assistant\", \"content\": \"It is green.\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb462d7",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4103aea9",
   "metadata": {},
   "source": [
    "* 일단 기존 방식대로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6c130",
   "metadata": {},
   "source": [
    "## DPO\n",
    "\n",
    "DPOTrainer 소스 코드 https://github.com/huggingface/trl/blob/d625c5533a6b1c84d3565c8080857f6bb81c538a/trl/trainer/dpo_trainer.py#L1145-L1149\n",
    "\n",
    "* SFT보다 learning_rate를 훨씬 작게 설정해줘야 함\n",
    "* packing은 쌍으로 존재하는 데이터에서 불가능. padding_free는 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d48a0d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a ManyToManyField in Django, you need to define a ManyToManyField on both models involved in the relationship.\n",
      "\n",
      "Assuming you already have the models for which you want to create a many-to-many relationship, you can add the ManyToManyField as follows:\n",
      "\n",
      "1. In the model where you want to define the many-to-many field, add the following line of code:\n",
      "```python\n",
      "class MyModel(models.Model):\n",
      "    # fields and other descriptors for MyModel\n",
      "    related_models = models.ManyToManyField('OtherModel', related_name='my_model_set')\n",
      "```\n",
      "Replace 'OtherModel' with the name of the model you want to create a many-to-many relationship with, and'my\\_model\\_set' with the name you want to use for the reverse relationship on the other model.\n",
      "\n",
      "2. In the model where you want to define the reverse many-to-many field, add the following line of code:\n",
      "```python\n",
      "class OtherModel(models.Model):\n",
      "    # fields and other descriptors for OtherModel\n",
      "    my_model = models.ManyToManyField('MyModel', related_name='other_model_set')\n",
      "```\n",
      "This creates a many-to-many relationship between the two models, and you can use the field in your model's administration page to manage the relationship.<|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dpo_trainer.train_dataset[0][\"chosen_input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2aa41",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ede2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc696f",
   "metadata": {},
   "source": [
    "`-` 테스트에 사용될 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b61de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files = os.path.join(\"\", \"data/dpo_test_dataset.json\"), split = \"train\")\n",
    "random_idx = randint(0, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ce6ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: come up with a multiple choice question from a random topic and ask me, awarding me points starting with 1 point per question, and increasing the number of points per question by 1 for the first five questions and reaching a max of 10 points per question. \n",
      "Ask 20 questions, one at a time and don't tell me the answer until you get my guess and check with the correct answer. \n",
      "Pretend you are a game show host with every response and say a stupid pun related to the question after each response.\n",
      "\n",
      "\n",
      "chosen: Welcome to \"Quiz Your Brain,\" the game show where knowledge is power, and puns are plentiful! Are you ready to test your wisdom and have a groan-worthy good time? Let's get started!\n",
      "\n",
      "Question 1: In which European country would you find the city of Prague? (a) France, (b) Germany, (c) Italy, or (d) Czechia. Remember, it's a beautiful “praise” for your geographical knowledge!\n",
      "\n",
      "*Points: 1\n",
      "\n",
      "\n",
      "rejected: Sure, I can do that. Here's the first question:\n",
      "\n",
      "What is the capital of France?\n",
      "\n",
      "(1 point)\n",
      "(A) Paris\n",
      "(B) Berlin\n",
      "(C) Rome\n",
      "(D) London\n",
      "\n",
      "And the answer is...\n",
      "\n",
      "**Paris**! That's right, the capital of France is Paris. I hope you got that one right, because you'll get 1 point for it. Now, on to the next question.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in test_dataset[random_idx].items():\n",
    "    print(f\"{k}: {v[0][\"content\"]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68ac67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = test_dataset[random_idx][\"prompt\"]\n",
    "chosen = test_dataset[random_idx][\"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac172034",
   "metadata": {},
   "source": [
    "`-` 원시 모델 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0f7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.39s/it]\n"
     ]
    }
   ],
   "source": [
    "origin_model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "origin_model = AutoModelForCausalLM.from_pretrained(origin_model_name, use_cache = False, device_map = \"cuda:0\", dtype = torch.bfloat16)\n",
    "origin_tokenizer = AutoTokenizer.from_pretrained(origin_model_name, use_fast = True)\n",
    "origin_tokenizer.pad_token = origin_tokenizer.eos_token\n",
    "origin_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4515a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "come up with a multiple choice question from a random topic and ask me, awarding me points starting with 1 point per question, and increasing the number of points per question by 1 for the first five questions and reaching a max of 10 points per question. \n",
      "Ask 20 questions, one at a time and don't tell me the answer until you get my guess and check with the correct answer. \n",
      "Pretend you are a game show host with every response and say a stupid pun related to the question after each response.\n",
      "\n",
      "chosen:\n",
      "Welcome to \"Quiz Your Brain,\" the game show where knowledge is power, and puns are plentiful! Are you ready to test your wisdom and have a groan-worthy good time? Let's get started!\n",
      "\n",
      "Question 1: In which European country would you find the city of Prague? (a) France, (b) Germany, (c) Italy, or (d) Czechia. Remember, it's a beautiful “praise” for your geographical knowledge!\n",
      "\n",
      "*Points: 1\n",
      "\n",
      "generate:\n",
      "Welcome to \"Quiz Up,\" the game show where knowledge is power and puns are the spark plugs that ignite the excitement! You'll be competing for a whopping 20 questions, with the points increasing like a snowball rolling down a hill. \n",
      "\n",
      "Here's your first question, worth 1 point. Let's get this quiz started!\n",
      "\n",
      "Question 1: What is the largest planet in our solar system?\n",
      "\n",
      "A) Earth\n",
      "B) Saturn\n",
      "C) Jupiter\n",
      "D) Uranus\n",
      "\n",
      "Take your time, and remember, the correct answer is out of this world!\n"
     ]
    }
   ],
   "source": [
    "terminators = [origin_tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = origin_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(origin_model.device)\n",
    "\n",
    "outputs = origin_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{origin_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df70337",
   "metadata": {},
   "outputs": [],
   "source": [
    "del origin_model\n",
    "del origin_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69744d39",
   "metadata": {},
   "source": [
    "`-` 파인튜닝 후 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b555ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.71s/it]\n"
     ]
    }
   ],
   "source": [
    "sft_adapter_name = \"./results/test\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "config = PeftConfig.from_pretrained(sft_adapter_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config = bnb_config,\n",
    "    use_cache = True,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"cuda:0\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, sft_adapter_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_adapter_name, use_fast = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "045302ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "come up with a multiple choice question from a random topic and ask me, awarding me points starting with 1 point per question, and increasing the number of points per question by 1 for the first five questions and reaching a max of 10 points per question. \n",
      "Ask 20 questions, one at a time and don't tell me the answer until you get my guess and check with the correct answer. \n",
      "Pretend you are a game show host with every response and say a stupid pun related to the question after each response.\n",
      "\n",
      "chosen:\n",
      "Welcome to \"Quiz Your Brain,\" the game show where knowledge is power, and puns are plentiful! Are you ready to test your wisdom and have a groan-worthy good time? Let's get started!\n",
      "\n",
      "Question 1: In which European country would you find the city of Prague? (a) France, (b) Germany, (c) Italy, or (d) Czechia. Remember, it's a beautiful “praise” for your geographical knowledge!\n",
      "\n",
      "*Points: 1\n",
      "\n",
      "generate:\n",
      "Sure, I'd be happy to play a game with you! Here's my first question:\n",
      "\n",
      "What is the world's largest living structure, according to the Guinness World Records?\n",
      "\n",
      "a) The Great Barrier Reef\n",
      "b) The Amazon Rainforest\n",
      "c) The Grand Canyon\n",
      "d) The Great Wall of China\n",
      "\n",
      "You earn 1 point for answering this question.\n",
      "\n",
      "And here's a pun to get you started: \"I'm here to help you with your questions, and I'm a-lot of fun to talk to!\"\n"
     ]
    }
   ],
   "source": [
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87615ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7c2bd",
   "metadata": {},
   "source": [
    "`-` DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac83166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.31s/it]\n"
     ]
    }
   ],
   "source": [
    "dpo_adapter_name = \"./results/dpo-reuse/checkpoint-6854\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "config = PeftConfig.from_pretrained(f\"{dpo_adapter_name}/policy\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config = bnb_config,\n",
    "    use_cache = True,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = \"cuda:0\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, f\"{dpo_adapter_name}/policy\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(dpo_adapter_name, use_fast = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e5dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "come up with a multiple choice question from a random topic and ask me, awarding me points starting with 1 point per question, and increasing the number of points per question by 1 for the first five questions and reaching a max of 10 points per question. \n",
      "Ask 20 questions, one at a time and don't tell me the answer until you get my guess and check with the correct answer. \n",
      "Pretend you are a game show host with every response and say a stupid pun related to the question after each response.\n",
      "\n",
      "chosen:\n",
      "Welcome to \"Quiz Your Brain,\" the game show where knowledge is power, and puns are plentiful! Are you ready to test your wisdom and have a groan-worthy good time? Let's get started!\n",
      "\n",
      "Question 1: In which European country would you find the city of Prague? (a) France, (b) Germany, (c) Italy, or (d) Czechia. Remember, it's a beautiful “praise” for your geographical knowledge!\n",
      "\n",
      "*Points: 1\n",
      "\n",
      "생성답변:\n",
      "Host: Welcome to the exciting game of \"Trivia Time!\" where you can test your knowledge and earn points! Let's get started with your first question, worth 1 point, and make it a fun one, shall we?\n",
      "\n",
      "Question 1: What is the world's largest living structure, according to the Guinness World Records?\n",
      "\n",
      "A) The Great Barrier Reef\n",
      "B) The Amazon Rainforest\n",
      "C) The Grand Canyon\n",
      "D) The Great Wall of China\n",
      "\n",
      "Host: Take your time, think carefully, and remember, the answer is out there, waiting to be discovered!\n",
      "\n",
      "What's your answer?\n"
     ]
    }
   ],
   "source": [
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"생성답변:\\n{tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd962f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dpo_model\n",
    "del dpo_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c4190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
