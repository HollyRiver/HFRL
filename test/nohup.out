/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
======== Log a few random samples from the processed training set ========
Traceback (most recent call last):
  File "/root/HFRL/test/SFT_test.py", line 199, in <module>
    main(script_args, training_args, lora_kwargs)
  File "/root/HFRL/test/SFT_test.py", line 52, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/root/HFRL/test/SFT_test.py", line 121, in main
    print(tokenizer.apply_chat_templaet(train_ds[index]["messages"], tokenize = False))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 1127, in __getattr__
    raise AttributeError(f"{self.__class__.__name__} has no attribute {key}")
AttributeError: PreTrainedTokenizerFast has no attribute apply_chat_templaet. Did you mean: 'apply_chat_template'?
/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
======== Log a few random samples from the processed training set ========
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>

왜 딸기나 바나나 쥬스는 없고, 우유로 된 제품만 있을까요? 
딸기우유나 바나나우유는 왜 쥬스로 만들어지지 않을까요? 사과쥬스나 오렌지쥬스, 포도쥬스는 많은데 딸기나 바나나 쥬스가 없다는 점이 궁금합니다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

딸기나 바나나 같은 과일은 먼저 수확하여 품질이 좋은 것들을 가지고 주스를 만들기는 어렵습니다. 생산성이 좋지 않아 그대로 판매하는 것이 경제적입니다. 또한 농축액을 만들 때 과일 수분과 과당의 양이 중요한데, 딸기나 바나나 같은 경우 농축액을 만드는 것이 어렵고 경제적이지 않습니다. 게다가 생과일쥬스나 아이스크림 등 이미 다양한 과일 제품이 있으며, 이들을 대량으로 생산하는 것이 기업 입장에서 경쟁력도 없고 이익도 적습니다. 하지만 유전자 공학이 발전하면서 더욱 질 좋은 농축액을 만들어낼 수 있는 딸기나 바나나 품종이 개발될 수도 있기 때문에 희망을 가지실 수 있습니다.<|eot_id|>
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>

우주에서는 선풍기나 헬리콥터, 비행기가 날아다닐 수 없는 이유는 무엇일까요? 로켓은 공기가 없어도 작동하는 이유는 무엇일까요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

우리가 지구 대기권을 벗어난 공간, 즉 우주는 공기가 전혀 없는 상태에 가깝습니다. 따라서 바람도 불지 않고, 공기중의 산소와 연료로 동력을 얻는 선풍기나 헬리콥터, 비행기는 날아다닐 수 없습니다. 그러나 로켓은 내부에 연료에 함유된 산소를 이용해 연소를 일으키므로 공기가 없어도 작동이 가능합니다. 즉, 로켓이 대기권 밖에서도 작동 가능한 이유는 내부에 있는 산소를 이용한다는 것입니다.<|eot_id|>

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.85s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.22s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  4.72s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.63s/it]

Tokenizing train dataset:   0%|          | 0/19039 [00:00<?, ? examples/s]
Tokenizing train dataset:   0%|          | 33/19039 [00:00<00:59, 317.58 examples/s]
Tokenizing train dataset:   1%|          | 98/19039 [00:00<00:37, 503.10 examples/s]
Tokenizing train dataset:   1%|          | 166/19039 [00:00<00:32, 579.52 examples/s]
Tokenizing train dataset:   1%|          | 237/19039 [00:00<00:30, 625.69 examples/s]
Tokenizing train dataset:   2%|▏         | 303/19039 [00:00<00:29, 635.96 examples/s]
Tokenizing train dataset:   2%|▏         | 386/19039 [00:00<00:32, 582.50 examples/s]
Tokenizing train dataset:   2%|▏         | 450/19039 [00:00<00:31, 595.55 examples/s]
Tokenizing train dataset:   3%|▎         | 523/19039 [00:00<00:29, 630.88 examples/s]
Tokenizing train dataset:   3%|▎         | 621/19039 [00:01<00:28, 636.95 examples/s]
Tokenizing train dataset:   4%|▎         | 700/19039 [00:01<00:27, 675.25 examples/s]
Tokenizing train dataset:   4%|▍         | 777/19039 [00:01<00:26, 695.14 examples/s]
Tokenizing train dataset:   5%|▍         | 878/19039 [00:01<00:26, 684.06 examples/s]
Tokenizing train dataset:   5%|▌         | 981/19039 [00:01<00:26, 680.80 examples/s]
Tokenizing train dataset:   6%|▌         | 1069/19039 [00:07<06:02, 49.63 examples/s]
Tokenizing train dataset:   6%|▌         | 1147/19039 [00:07<04:28, 66.60 examples/s]
Tokenizing train dataset:   6%|▋         | 1223/19039 [00:07<03:20, 88.89 examples/s]
Tokenizing train dataset:   7%|▋         | 1298/19039 [00:07<02:30, 117.90 examples/s]
Tokenizing train dataset:   7%|▋         | 1375/19039 [00:07<01:53, 156.28 examples/s]
Tokenizing train dataset:   8%|▊         | 1449/19039 [00:07<01:27, 201.28 examples/s]
Tokenizing train dataset:   8%|▊         | 1519/19039 [00:07<01:09, 250.66 examples/s]
Tokenizing train dataset:   8%|▊         | 1602/19039 [00:07<00:54, 322.35 examples/s]
Tokenizing train dataset:   9%|▉         | 1687/19039 [00:08<00:43, 401.02 examples/s]
Tokenizing train dataset:   9%|▉         | 1765/19039 [00:08<00:36, 467.41 examples/s]
Tokenizing train dataset:  10%|▉         | 1875/19039 [00:08<00:31, 538.03 examples/s]
Tokenizing train dataset:  10%|█         | 1987/19039 [00:08<00:28, 594.60 examples/s]
Tokenizing train dataset:  11%|█         | 2091/19039 [00:09<00:54, 311.36 examples/s]
Tokenizing train dataset:  11%|█▏        | 2175/19039 [00:09<00:45, 373.86 examples/s]
Tokenizing train dataset:  12%|█▏        | 2248/19039 [00:09<00:39, 424.54 examples/s]
Tokenizing train dataset:  12%|█▏        | 2321/19039 [00:09<00:35, 475.50 examples/s]
Tokenizing train dataset:  13%|█▎        | 2396/19039 [00:09<00:31, 527.10 examples/s]
Tokenizing train dataset:  13%|█▎        | 2480/19039 [00:09<00:27, 593.96 examples/s]
Tokenizing train dataset:  14%|█▎        | 2584/19039 [00:09<00:26, 622.58 examples/s]
Tokenizing train dataset:  14%|█▍        | 2663/19039 [00:09<00:24, 659.79 examples/s]
Tokenizing train dataset:  14%|█▍        | 2744/19039 [00:09<00:23, 695.09 examples/s]
Tokenizing train dataset:  15%|█▍        | 2851/19039 [00:10<00:23, 699.17 examples/s]
Tokenizing train dataset:  15%|█▌        | 2937/19039 [00:10<00:21, 737.07 examples/s]
Tokenizing train dataset:  16%|█▌        | 3037/19039 [00:18<07:00, 38.08 examples/s] 
Tokenizing train dataset:  16%|█▋        | 3111/19039 [00:18<05:17, 50.21 examples/s]
Tokenizing train dataset:  17%|█▋        | 3185/19039 [00:18<03:57, 66.82 examples/s]
Tokenizing train dataset:  17%|█▋        | 3263/19039 [00:18<02:54, 90.52 examples/s]
Tokenizing train dataset:  18%|█▊        | 3341/19039 [00:18<02:08, 121.72 examples/s]
Tokenizing train dataset:  18%|█▊        | 3415/19039 [00:18<01:38, 159.17 examples/s]
Tokenizing train dataset:  18%|█▊        | 3489/19039 [00:18<01:15, 205.47 examples/s]
Tokenizing train dataset:  19%|█▉        | 3578/19039 [00:18<00:56, 275.39 examples/s]
Tokenizing train dataset:  19%|█▉        | 3657/19039 [00:18<00:45, 340.10 examples/s]
Tokenizing train dataset:  20%|█▉        | 3768/19039 [00:19<00:36, 424.06 examples/s]
Tokenizing train dataset:  20%|██        | 3854/19039 [00:19<00:30, 497.00 examples/s]
Tokenizing train dataset:  21%|██        | 3935/19039 [00:19<00:27, 557.23 examples/s]
Tokenizing train dataset:  21%|██        | 4038/19039 [00:19<00:33, 453.68 examples/s]
Tokenizing train dataset:  22%|██▏       | 4121/19039 [00:19<00:28, 519.45 examples/s]
Tokenizing train dataset:  22%|██▏       | 4194/19039 [00:19<00:26, 558.67 examples/s]
Tokenizing train dataset:  22%|██▏       | 4273/19039 [00:19<00:24, 607.98 examples/s]
Tokenizing train dataset:  23%|██▎       | 4358/19039 [00:19<00:22, 664.68 examples/s]
Tokenizing train dataset:  23%|██▎       | 4465/19039 [00:20<00:21, 679.47 examples/s]
Tokenizing train dataset:  24%|██▍       | 4549/19039 [00:20<00:20, 715.86 examples/s]
Tokenizing train dataset:  24%|██▍       | 4632/19039 [00:20<00:19, 742.90 examples/s]
Tokenizing train dataset:  25%|██▍       | 4747/19039 [00:20<00:19, 749.23 examples/s]
Tokenizing train dataset:  25%|██▌       | 4836/19039 [00:20<00:18, 783.57 examples/s]
Tokenizing train dataset:  26%|██▌       | 4948/19039 [00:20<00:18, 767.41 examples/s]
Tokenizing train dataset:  26%|██▋       | 5035/19039 [00:20<00:24, 567.26 examples/s]
Tokenizing train dataset:  27%|██▋       | 5113/19039 [00:21<00:22, 609.32 examples/s]
Tokenizing train dataset:  27%|██▋       | 5194/19039 [00:21<00:21, 653.31 examples/s]
Tokenizing train dataset:  28%|██▊       | 5267/19039 [00:21<00:20, 669.18 examples/s]
Tokenizing train dataset:  28%|██▊       | 5348/19039 [00:21<00:19, 703.13 examples/s]
Tokenizing train dataset:  29%|██▊       | 5435/19039 [00:21<00:18, 746.51 examples/s]
Tokenizing train dataset:  29%|██▉       | 5545/19039 [00:21<00:18, 738.92 examples/s]
Tokenizing train dataset:  30%|██▉       | 5630/19039 [00:21<00:17, 765.02 examples/s]
Tokenizing train dataset:  30%|███       | 5712/19039 [00:21<00:17, 778.77 examples/s]
Tokenizing train dataset:  31%|███       | 5823/19039 [00:21<00:17, 758.74 examples/s]
Tokenizing train dataset:  31%|███       | 5904/19039 [00:22<00:17, 770.59 examples/s]
Tokenizing train dataset:  31%|███▏      | 5991/19039 [00:22<00:16, 794.47 examples/s]
Tokenizing train dataset:  32%|███▏      | 6111/19039 [00:22<00:22, 572.76 examples/s]
Tokenizing train dataset:  33%|███▎      | 6194/19039 [00:22<00:20, 622.64 examples/s]
Tokenizing train dataset:  33%|███▎      | 6272/19039 [00:22<00:19, 654.47 examples/s]
Tokenizing train dataset:  33%|███▎      | 6350/19039 [00:22<00:18, 682.76 examples/s]
Tokenizing train dataset:  34%|███▍      | 6439/19039 [00:22<00:17, 731.59 examples/s]
Tokenizing train dataset:  34%|███▍      | 6551/19039 [00:23<00:17, 731.64 examples/s]
Tokenizing train dataset:  35%|███▍      | 6637/19039 [00:23<00:16, 762.35 examples/s]
Tokenizing train dataset:  35%|███▌      | 6752/19039 [00:23<00:16, 762.04 examples/s]
Tokenizing train dataset:  36%|███▌      | 6841/19039 [00:24<00:42, 286.92 examples/s]
Tokenizing train dataset:  36%|███▋      | 6921/19039 [00:24<00:35, 344.81 examples/s]
Tokenizing train dataset:  37%|███▋      | 6994/19039 [00:24<00:30, 398.06 examples/s]
Tokenizing train dataset:  37%|███▋      | 7083/19039 [00:24<00:31, 383.17 examples/s]
Tokenizing train dataset:  38%|███▊      | 7167/19039 [00:24<00:26, 455.22 examples/s]
Tokenizing train dataset:  38%|███▊      | 7240/19039 [00:24<00:23, 504.68 examples/s]
Tokenizing train dataset:  38%|███▊      | 7309/19039 [00:24<00:21, 541.11 examples/s]
Tokenizing train dataset:  39%|███▉      | 7380/19039 [00:24<00:20, 578.07 examples/s]
Tokenizing train dataset:  39%|███▉      | 7458/19039 [00:25<00:18, 626.44 examples/s]
Tokenizing train dataset:  40%|███▉      | 7539/19039 [00:25<00:17, 670.14 examples/s]
Tokenizing train dataset:  40%|████      | 7628/19039 [00:25<00:15, 726.64 examples/s]
Tokenizing train dataset:  40%|████      | 7706/19039 [00:25<00:15, 739.22 examples/s]
Tokenizing train dataset:  41%|████      | 7828/19039 [00:25<00:14, 761.59 examples/s]
Tokenizing train dataset:  42%|████▏     | 7917/19039 [00:25<00:14, 793.12 examples/s]
Tokenizing train dataset:  42%|████▏     | 8000/19039 [00:26<00:25, 426.16 examples/s]
Tokenizing train dataset:  42%|████▏     | 8072/19039 [00:26<00:23, 475.42 examples/s]
Tokenizing train dataset:  43%|████▎     | 8153/19039 [00:26<00:20, 539.02 examples/s]
Tokenizing train dataset:  43%|████▎     | 8260/19039 [00:26<00:18, 587.57 examples/s]
Tokenizing train dataset:  44%|████▍     | 8338/19039 [00:26<00:17, 627.82 examples/s]
Tokenizing train dataset:  44%|████▍     | 8420/19039 [00:26<00:15, 670.94 examples/s]
Tokenizing train dataset:  45%|████▍     | 8495/19039 [00:26<00:15, 689.73 examples/s]
Tokenizing train dataset:  45%|████▌     | 8572/19039 [00:26<00:14, 710.40 examples/s]
Tokenizing train dataset:  45%|████▌     | 8660/19039 [00:26<00:13, 755.00 examples/s]
Tokenizing train dataset:  46%|████▌     | 8762/19039 [00:27<00:14, 723.26 examples/s]
Tokenizing train dataset:  46%|████▋     | 8844/19039 [00:27<00:13, 746.71 examples/s]
Tokenizing train dataset:  47%|████▋     | 8926/19039 [00:27<00:13, 765.39 examples/s]
Tokenizing train dataset:  47%|████▋     | 9037/19039 [00:28<00:44, 224.18 examples/s]
Tokenizing train dataset:  48%|████▊     | 9120/19039 [00:28<00:35, 280.36 examples/s]
Tokenizing train dataset:  48%|████▊     | 9191/19039 [00:28<00:29, 330.30 examples/s]
Tokenizing train dataset:  49%|████▊     | 9270/19039 [00:28<00:24, 395.81 examples/s]
Tokenizing train dataset:  49%|████▉     | 9352/19039 [00:28<00:20, 466.98 examples/s]
Tokenizing train dataset:  50%|████▉     | 9466/19039 [00:28<00:17, 544.15 examples/s]
Tokenizing train dataset:  50%|█████     | 9552/19039 [00:29<00:15, 606.20 examples/s]
Tokenizing train dataset:  51%|█████     | 9664/19039 [00:29<00:14, 646.96 examples/s]
Tokenizing train dataset:  51%|█████     | 9755/19039 [00:29<00:13, 702.76 examples/s]
Tokenizing train dataset:  52%|█████▏    | 9837/19039 [00:29<00:12, 729.87 examples/s]
Tokenizing train dataset:  52%|█████▏    | 9958/19039 [00:29<00:12, 752.14 examples/s]
Tokenizing train dataset:  53%|█████▎    | 10076/19039 [00:29<00:15, 587.62 examples/s]
Tokenizing train dataset:  53%|█████▎    | 10161/19039 [00:29<00:13, 637.46 examples/s]
Tokenizing train dataset:  54%|█████▍    | 10269/19039 [00:30<00:13, 658.88 examples/s]
Tokenizing train dataset:  54%|█████▍    | 10343/19039 [00:30<00:12, 674.45 examples/s]
Tokenizing train dataset:  55%|█████▍    | 10417/19039 [00:30<00:12, 688.42 examples/s]
Tokenizing train dataset:  55%|█████▌    | 10506/19039 [00:30<00:11, 738.19 examples/s]
Tokenizing train dataset:  56%|█████▌    | 10587/19039 [00:30<00:11, 755.68 examples/s]
Tokenizing train dataset:  56%|█████▌    | 10703/19039 [00:30<00:10, 758.38 examples/s]
Tokenizing train dataset:  57%|█████▋    | 10789/19039 [00:30<00:10, 782.84 examples/s]
Tokenizing train dataset:  57%|█████▋    | 10904/19039 [00:30<00:10, 775.47 examples/s]
Tokenizing train dataset:  58%|█████▊    | 11000/19039 [00:31<00:14, 561.10 examples/s]
Tokenizing train dataset:  58%|█████▊    | 11076/19039 [00:31<00:13, 599.15 examples/s]
Tokenizing train dataset:  59%|█████▊    | 11151/19039 [00:31<00:12, 628.94 examples/s]
Tokenizing train dataset:  59%|█████▉    | 11237/19039 [00:31<00:11, 682.90 examples/s]
Tokenizing train dataset:  59%|█████▉    | 11319/19039 [00:31<00:10, 716.51 examples/s]
Tokenizing train dataset:  60%|█████▉    | 11420/19039 [00:31<00:10, 697.77 examples/s]
Tokenizing train dataset:  60%|██████    | 11495/19039 [00:31<00:10, 706.94 examples/s]
Tokenizing train dataset:  61%|██████    | 11580/19039 [00:31<00:10, 742.44 examples/s]
Tokenizing train dataset:  61%|██████    | 11657/19039 [00:32<00:09, 747.73 examples/s]
Tokenizing train dataset:  62%|██████▏   | 11771/19039 [00:32<00:09, 749.15 examples/s]
Tokenizing train dataset:  62%|██████▏   | 11856/19039 [00:32<00:09, 774.10 examples/s]
Tokenizing train dataset:  63%|██████▎   | 11938/19039 [00:32<00:09, 783.63 examples/s]
Tokenizing train dataset:  63%|██████▎   | 12035/19039 [00:33<00:23, 297.00 examples/s]
Tokenizing train dataset:  64%|██████▎   | 12119/19039 [00:33<00:19, 362.95 examples/s]
Tokenizing train dataset:  64%|██████▍   | 12200/19039 [00:33<00:15, 429.58 examples/s]
Tokenizing train dataset:  64%|██████▍   | 12272/19039 [00:33<00:14, 479.53 examples/s]
Tokenizing train dataset:  65%|██████▍   | 12344/19039 [00:33<00:12, 525.98 examples/s]
Tokenizing train dataset:  65%|██████▌   | 12418/19039 [00:33<00:11, 572.22 examples/s]
Tokenizing train dataset:  66%|██████▌   | 12491/19039 [00:33<00:10, 608.93 examples/s]
Tokenizing train dataset:  66%|██████▌   | 12573/19039 [00:33<00:09, 661.53 examples/s]
Tokenizing train dataset:  67%|██████▋   | 12665/19039 [00:33<00:08, 727.42 examples/s]
Tokenizing train dataset:  67%|██████▋   | 12780/19039 [00:34<00:08, 738.66 examples/s]
Tokenizing train dataset:  68%|██████▊   | 12893/19039 [00:34<00:08, 740.35 examples/s]
Tokenizing train dataset:  68%|██████▊   | 13000/19039 [00:34<00:10, 570.28 examples/s]
Tokenizing train dataset:  69%|██████▊   | 13071/19039 [00:34<00:10, 596.21 examples/s]
Tokenizing train dataset:  69%|██████▉   | 13156/19039 [00:34<00:09, 649.82 examples/s]
Tokenizing train dataset:  70%|██████▉   | 13260/19039 [00:34<00:08, 661.06 examples/s]
Tokenizing train dataset:  70%|███████   | 13344/19039 [00:34<00:08, 701.80 examples/s]
Tokenizing train dataset:  70%|███████   | 13419/19039 [00:35<00:07, 712.90 examples/s]
Tokenizing train dataset:  71%|███████   | 13495/19039 [00:35<00:07, 722.10 examples/s]
Tokenizing train dataset:  71%|███████▏  | 13570/19039 [00:35<00:07, 727.40 examples/s]
Tokenizing train dataset:  72%|███████▏  | 13649/19039 [00:35<00:07, 740.49 examples/s]
Tokenizing train dataset:  72%|███████▏  | 13761/19039 [00:35<00:07, 735.47 examples/s]
Tokenizing train dataset:  73%|███████▎  | 13873/19039 [00:35<00:07, 737.22 examples/s]
Tokenizing train dataset:  73%|███████▎  | 13953/19039 [00:35<00:06, 747.48 examples/s]
Tokenizing train dataset:  74%|███████▍  | 14043/19039 [00:36<00:08, 571.63 examples/s]
Tokenizing train dataset:  74%|███████▍  | 14113/19039 [00:36<00:08, 595.52 examples/s]
Tokenizing train dataset:  74%|███████▍  | 14181/19039 [00:36<00:07, 613.99 examples/s]
Tokenizing train dataset:  75%|███████▍  | 14256/19039 [00:36<00:07, 645.54 examples/s]
Tokenizing train dataset:  75%|███████▌  | 14328/19039 [00:36<00:07, 663.12 examples/s]
Tokenizing train dataset:  76%|███████▌  | 14403/19039 [00:36<00:06, 683.42 examples/s]
Tokenizing train dataset:  76%|███████▌  | 14479/19039 [00:36<00:06, 702.47 examples/s]
Tokenizing train dataset:  76%|███████▋  | 14552/19039 [00:36<00:06, 707.16 examples/s]
Tokenizing train dataset:  77%|███████▋  | 14630/19039 [00:36<00:06, 723.89 examples/s]
Tokenizing train dataset:  77%|███████▋  | 14744/19039 [00:37<00:05, 734.86 examples/s]
Tokenizing train dataset:  78%|███████▊  | 14836/19039 [00:37<00:05, 781.43 examples/s]
Tokenizing train dataset:  78%|███████▊  | 14915/19039 [00:37<00:05, 780.55 examples/s]
Tokenizing train dataset:  79%|███████▉  | 15000/19039 [00:37<00:07, 557.21 examples/s]
Tokenizing train dataset:  79%|███████▉  | 15067/19039 [00:37<00:06, 580.44 examples/s]
Tokenizing train dataset:  80%|███████▉  | 15142/19039 [00:37<00:06, 620.00 examples/s]
Tokenizing train dataset:  80%|███████▉  | 15216/19039 [00:37<00:05, 648.18 examples/s]
Tokenizing train dataset:  80%|████████  | 15290/19039 [00:37<00:05, 672.26 examples/s]
Tokenizing train dataset:  81%|████████  | 15364/19039 [00:37<00:05, 688.95 examples/s]
Tokenizing train dataset:  81%|████████  | 15466/19039 [00:38<00:05, 682.77 examples/s]
Tokenizing train dataset:  82%|████████▏ | 15543/19039 [00:38<00:04, 701.62 examples/s]
Tokenizing train dataset:  82%|████████▏ | 15633/19039 [00:38<00:04, 751.73 examples/s]
Tokenizing train dataset:  83%|████████▎ | 15710/19039 [00:38<00:04, 754.79 examples/s]
Tokenizing train dataset:  83%|████████▎ | 15787/19039 [00:38<00:04, 756.11 examples/s]
Tokenizing train dataset:  83%|████████▎ | 15874/19039 [00:38<00:04, 784.12 examples/s]
Tokenizing train dataset:  84%|████████▍ | 15983/19039 [00:38<00:04, 755.25 examples/s]
Tokenizing train dataset:  84%|████████▍ | 16072/19039 [00:39<00:05, 544.50 examples/s]
Tokenizing train dataset:  85%|████████▍ | 16154/19039 [00:39<00:04, 599.85 examples/s]
Tokenizing train dataset:  85%|████████▌ | 16232/19039 [00:39<00:04, 638.24 examples/s]
Tokenizing train dataset:  86%|████████▌ | 16337/19039 [00:39<00:04, 655.49 examples/s]
Tokenizing train dataset:  86%|████████▋ | 16422/19039 [00:39<00:03, 697.97 examples/s]
Tokenizing train dataset:  87%|████████▋ | 16529/19039 [00:39<00:03, 700.33 examples/s]
Tokenizing train dataset:  87%|████████▋ | 16604/19039 [00:39<00:03, 708.10 examples/s]
Tokenizing train dataset:  88%|████████▊ | 16682/19039 [00:39<00:03, 725.33 examples/s]
Tokenizing train dataset:  88%|████████▊ | 16781/19039 [00:40<00:03, 697.61 examples/s]
Tokenizing train dataset:  89%|████████▊ | 16861/19039 [00:40<00:03, 721.91 examples/s]
Tokenizing train dataset:  89%|████████▉ | 16960/19039 [00:40<00:02, 697.05 examples/s]
Tokenizing train dataset:  89%|████████▉ | 17033/19039 [00:40<00:03, 509.81 examples/s]
Tokenizing train dataset:  90%|████████▉ | 17118/19039 [00:40<00:03, 577.52 examples/s]
Tokenizing train dataset:  90%|█████████ | 17196/19039 [00:40<00:02, 621.46 examples/s]
Tokenizing train dataset:  91%|█████████ | 17272/19039 [00:40<00:02, 651.57 examples/s]
Tokenizing train dataset:  91%|█████████ | 17344/19039 [00:40<00:02, 666.37 examples/s]
Tokenizing train dataset:  92%|█████████▏| 17426/19039 [00:41<00:02, 704.54 examples/s]
Tokenizing train dataset:  92%|█████████▏| 17507/19039 [00:41<00:02, 732.58 examples/s]
Tokenizing train dataset:  92%|█████████▏| 17584/19039 [00:41<00:01, 739.36 examples/s]
Tokenizing train dataset:  93%|█████████▎| 17671/19039 [00:41<00:01, 772.01 examples/s]
Tokenizing train dataset:  93%|█████████▎| 17755/19039 [00:41<00:01, 788.87 examples/s]
Tokenizing train dataset:  94%|█████████▍| 17871/19039 [00:41<00:01, 781.23 examples/s]
Tokenizing train dataset:  94%|█████████▍| 17952/19039 [00:41<00:01, 785.78 examples/s]
Tokenizing train dataset:  95%|█████████▍| 18035/19039 [00:41<00:01, 573.96 examples/s]
Tokenizing train dataset:  95%|█████████▌| 18118/19039 [00:42<00:01, 628.02 examples/s]
Tokenizing train dataset:  96%|█████████▌| 18201/19039 [00:42<00:01, 674.56 examples/s]
Tokenizing train dataset:  96%|█████████▌| 18276/19039 [00:42<00:01, 692.13 examples/s]
Tokenizing train dataset:  97%|█████████▋| 18386/19039 [00:42<00:00, 703.79 examples/s]
Tokenizing train dataset:  97%|█████████▋| 18462/19039 [00:42<00:00, 714.73 examples/s]
Tokenizing train dataset:  97%|█████████▋| 18552/19039 [00:42<00:00, 759.76 examples/s]
Tokenizing train dataset:  98%|█████████▊| 18663/19039 [00:42<00:00, 747.61 examples/s]
Tokenizing train dataset:  98%|█████████▊| 18744/19039 [00:42<00:00, 757.17 examples/s]
Tokenizing train dataset:  99%|█████████▉| 18831/19039 [00:42<00:00, 784.73 examples/s]
Tokenizing train dataset:  99%|█████████▉| 18934/19039 [00:43<00:00, 748.36 examples/s]
Tokenizing train dataset: 100%|██████████| 19039/19039 [00:43<00:00, 550.22 examples/s]
Tokenizing train dataset: 100%|██████████| 19039/19039 [00:43<00:00, 438.55 examples/s]

Packing train dataset:   0%|          | 0/19039 [00:00<?, ? examples/s]
Packing train dataset:  16%|█▌        | 3000/19039 [00:00<00:00, 21887.82 examples/s]
Packing train dataset:  47%|████▋     | 9000/19039 [00:00<00:00, 36547.25 examples/s]
Packing train dataset:  79%|███████▉  | 15000/19039 [00:00<00:00, 40731.52 examples/s]
Packing train dataset: 100%|██████████| 19039/19039 [00:00<00:00, 29591.44 examples/s]

Tokenizing eval dataset:   0%|          | 0/2116 [00:00<?, ? examples/s]
Tokenizing eval dataset:   3%|▎         | 74/2116 [00:00<00:02, 727.63 examples/s]
Tokenizing eval dataset:   9%|▉         | 187/2116 [00:00<00:02, 739.00 examples/s]
Tokenizing eval dataset:  13%|█▎        | 269/2116 [00:00<00:02, 767.00 examples/s]
Tokenizing eval dataset:  16%|█▋        | 347/2116 [00:00<00:02, 768.45 examples/s]
Tokenizing eval dataset:  21%|██▏       | 452/2116 [00:00<00:02, 734.21 examples/s]
Tokenizing eval dataset:  26%|██▌       | 540/2116 [00:00<00:02, 772.88 examples/s]
Tokenizing eval dataset:  29%|██▉       | 621/2116 [00:00<00:01, 783.03 examples/s]
Tokenizing eval dataset:  35%|███▍      | 739/2116 [00:00<00:01, 778.62 examples/s]
Tokenizing eval dataset:  39%|███▊      | 818/2116 [00:01<00:01, 776.21 examples/s]
Tokenizing eval dataset:  43%|████▎     | 902/2116 [00:01<00:01, 791.26 examples/s]
Tokenizing eval dataset:  46%|████▋     | 982/2116 [00:01<00:01, 789.91 examples/s]
Tokenizing eval dataset:  51%|█████     | 1073/2116 [00:01<00:01, 552.60 examples/s]
Tokenizing eval dataset:  55%|█████▍    | 1160/2116 [00:01<00:01, 620.59 examples/s]
Tokenizing eval dataset:  59%|█████▊    | 1241/2116 [00:01<00:01, 662.71 examples/s]
Tokenizing eval dataset:  62%|██████▏   | 1317/2116 [00:01<00:01, 682.68 examples/s]
Tokenizing eval dataset:  66%|██████▋   | 1403/2116 [00:01<00:00, 726.24 examples/s]
Tokenizing eval dataset:  70%|███████   | 1484/2116 [00:02<00:00, 745.95 examples/s]
Tokenizing eval dataset:  75%|███████▌  | 1590/2116 [00:02<00:00, 727.35 examples/s]
Tokenizing eval dataset:  79%|███████▉  | 1675/2116 [00:02<00:00, 757.52 examples/s]
Tokenizing eval dataset:  83%|████████▎ | 1759/2116 [00:02<00:00, 774.33 examples/s]
Tokenizing eval dataset:  89%|████████▊ | 1875/2116 [00:02<00:00, 771.68 examples/s]
Tokenizing eval dataset:  92%|█████████▏| 1957/2116 [00:02<00:00, 780.43 examples/s]
Tokenizing eval dataset:  98%|█████████▊| 2067/2116 [00:02<00:00, 573.00 examples/s]
Tokenizing eval dataset: 100%|██████████| 2116/2116 [00:03<00:00, 691.94 examples/s]

Packing eval dataset:   0%|          | 0/2116 [00:00<?, ? examples/s]
Packing eval dataset: 100%|██████████| 2116/2116 [00:00<00:00, 35081.16 examples/s]
======== Log a first sample from the processed training set ========
masking area: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
wandb: Tracking run with wandb version 0.22.2
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /root/HFRL/test/wandb/offline-run-20251022_133206-mtff03kt
labels: tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],
       device='cuda:0') ... tensor([ 48424, 103079,  18359,  41820,  44005, 111436,  16969, 120078,     13,
        128009], device='cuda:0')

  0%|          | 0/4724 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.

  0%|          | 1/4724 [00:06<9:06:24,  6.94s/it]
  0%|          | 2/4724 [00:11<7:33:02,  5.76s/it]
  0%|          | 3/4724 [00:16<6:59:15,  5.33s/it]
  0%|          | 4/4724 [00:21<6:47:06,  5.18s/it]
  0%|          | 5/4724 [00:26<6:42:02,  5.11s/it]
  0%|          | 6/4724 [00:31<6:32:11,  4.99s/it]
  0%|          | 7/4724 [00:36<6:25:53,  4.91s/it]
  0%|          | 8/4724 [00:41<6:29:59,  4.96s/it]
  0%|          | 9/4724 [00:46<6:29:54,  4.96s/it]
  0%|          | 10/4724 [00:51<6:27:15,  4.93s/it]
  0%|          | 11/4724 [00:55<6:28:19,  4.94s/it]
  0%|          | 12/4724 [01:00<6:28:49,  4.95s/it]
  0%|          | 13/4724 [01:05<6:27:29,  4.94s/it]
  0%|          | 14/4724 [01:10<6:20:34,  4.85s/it]
  0%|          | 15/4724 [01:15<6:19:17,  4.83s/it]
  0%|          | 16/4724 [01:19<6:15:09,  4.78s/it]
  0%|          | 17/4724 [01:24<6:18:42,  4.83s/it]
  0%|          | 18/4724 [01:29<6:22:43,  4.88s/it]
  0%|          | 19/4724 [01:34<6:16:42,  4.80s/it]
  0%|          | 20/4724 [01:39<6:19:16,  4.84s/it]
  0%|          | 21/4724 [01:44<6:20:09,  4.85s/it]
  0%|          | 22/4724 [01:49<6:25:49,  4.92s/it]
  0%|          | 23/4724 [01:54<6:26:10,  4.93s/it]
  1%|          | 24/4724 [01:59<6:26:29,  4.93s/it]
  1%|          | 25/4724 [02:04<6:28:54,  4.97s/it]
  1%|          | 26/4724 [02:09<6:27:57,  4.95s/it]
  1%|          | 27/4724 [02:13<6:21:54,  4.88s/it]
  1%|          | 28/4724 [02:19<6:26:34,  4.94s/it]
  1%|          | 29/4724 [02:23<6:23:04,  4.90s/it]
  1%|          | 30/4724 [02:28<6:17:41,  4.83s/it]
  1%|          | 31/4724 [02:33<6:11:51,  4.75s/it]
  1%|          | 32/4724 [02:37<6:12:33,  4.76s/it]
  1%|          | 33/4724 [02:42<6:09:47,  4.73s/it]
  1%|          | 34/4724 [02:47<6:11:14,  4.75s/it]
  1%|          | 35/4724 [02:52<6:11:51,  4.76s/it]
  1%|          | 36/4724 [02:57<6:16:29,  4.82s/it]
  1%|          | 37/4724 [03:02<6:22:54,  4.90s/it]
  1%|          | 38/4724 [03:07<6:26:21,  4.95s/it]
  1%|          | 39/4724 [03:12<6:33:23,  5.04s/it]
  1%|          | 40/4724 [03:17<6:28:19,  4.97s/it]
  1%|          | 41/4724 [03:22<6:32:48,  5.03s/it]
  1%|          | 42/4724 [03:27<6:27:53,  4.97s/it]
  1%|          | 43/4724 [03:32<6:27:42,  4.97s/it]
  1%|          | 44/4724 [03:37<6:26:24,  4.95s/it]/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
======== Log a few random samples from the processed training set ========
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>

자동차 번호판은 어떻게 정해지나요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

자동차 번호판은 차량 등록지역과 차종/용도, 일련번호로 구성됩니다. 예를 들어, 서울 00 가 0000 의 번호판은 서울을 차량 등록지역으로 하며, 두자리 숫자 00은 차종/용도 번호를 의미합니다. 승용차는 11~69, 승합차는 70~79, 화물차는 80~97, 특수차는 98~99로 분류됩니다. 또한, 차량의 종류별로 한글 가나다라~~~에 따라 비사업용 자동차와 사업용 자동차를 구분합니다. 그 외에도 지자체마다 다른 번호의 정책을 가지고 있고, 대통령과 정부 고위 관료들이 타는 관용차량의 번호는 국가 기밀로 관리됩니다. 번호판 색상은 일반용이 녹색 바탕, 자가용 로프트(SOFA)이나 대여 차량은 황색 바탕에 남색 문자, 외교, 영사, 국제기구 등 특례 차량은 황색 바탕에 흑색 문자로 구성됩니다.색상은 사업용 차량은 모두 황색 바탕에 남색 문자를 사용합니다. 차종별 크기도 다르며, 임시 운행 기간에 사용되는 임시번호판도 있습니다.<|eot_id|>
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>

밀싹과 밀가루는 다른 작물인가요? 밀싹에 글루텐이 있나요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

밀싹과 밀가루는 같은 작물입니다. 밀가루는 익은 밀알에서 얻어지고, 밀싹은 밀알을 물에 담가 놓으면 싹이 나오는데 이것이 밀싹입니다. 즉, 벼심기 전에 벼싹이라고도 합니다. 밀싹에는 잎이 있기 때문에 글루텐이 없습니다.<|eot_id|>

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.17s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:15<00:15,  7.97s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.18s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  5.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.19s/it]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.
wandb: Tracking run with wandb version 0.22.2
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /root/HFRL/test/wandb/offline-run-20251022_133643-tlloa3x1
======== Log a first sample from the processed training set ========
masking area: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ...

  0%|          | 0/4724 [00:00<?, ?it/s]Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.

  0%|          | 1/4724 [00:05<7:50:31,  5.98s/it]
  0%|          | 2/4724 [00:10<7:04:04,  5.39s/it]
  0%|          | 3/4724 [00:15<6:45:35,  5.15s/it]
  0%|          | 4/4724 [00:20<6:33:54,  5.01s/it]
  0%|          | 5/4724 [00:25<6:33:15,  5.00s/it]
  0%|          | 6/4724 [00:30<6:31:18,  4.98s/it]
  0%|          | 7/4724 [00:35<6:25:57,  4.91s/it]
  0%|          | 8/4724 [00:40<6:25:41,  4.91s/it]
  0%|          | 9/4724 [00:44<6:22:31,  4.87s/it]
  0%|          | 10/4724 [00:50<6:30:32,  4.97s/it]
  0%|          | 11/4724 [00:55<6:30:25,  4.97s/it]
  0%|          | 12/4724 [01:00<6:31:48,  4.99s/it]
  0%|          | 13/4724 [01:05<6:33:08,  5.01s/it]
  0%|          | 14/4724 [01:09<6:26:55,  4.93s/it]
  0%|          | 15/4724 [01:14<6:17:54,  4.82s/it]
  0%|          | 16/4724 [01:19<6:17:44,  4.81s/it]
  0%|          | 17/4724 [01:24<6:21:27,  4.86s/it]
  0%|          | 18/4724 [01:29<6:21:00,  4.86s/it]
  0%|          | 19/4724 [01:33<6:18:50,  4.83s/it]
  0%|          | 20/4724 [01:38<6:20:15,  4.85s/it]
  0%|          | 21/4724 [01:43<6:19:58,  4.85s/it]
  0%|          | 22/4724 [01:48<6:24:42,  4.91s/it]
  0%|          | 23/4724 [01:53<6:23:22,  4.89s/it]
  1%|          | 24/4724 [01:58<6:23:21,  4.89s/it]
  1%|          | 25/4724 [02:03<6:25:13,  4.92s/it]
  1%|          | 26/4724 [02:08<6:28:37,  4.96s/it]
  1%|          | 27/4724 [02:13<6:22:49,  4.89s/it]
  1%|          | 28/4724 [02:18<6:20:05,  4.86s/it]
  1%|          | 29/4724 [02:22<6:17:40,  4.83s/it]
  1%|          | 30/4724 [02:27<6:17:41,  4.83s/it]
  1%|          | 31/4724 [02:32<6:15:03,  4.80s/it]
  1%|          | 32/4724 [02:36<6:10:23,  4.74s/it]
  1%|          | 33/4724 [02:41<6:13:27,  4.78s/it]
  1%|          | 34/4724 [02:46<6:08:04,  4.71s/it]
  1%|          | 35/4724 [02:51<6:12:03,  4.76s/it]
  1%|          | 36/4724 [02:56<6:15:18,  4.80s/it]
  1%|          | 37/4724 [03:01<6:21:39,  4.89s/it]
  1%|          | 38/4724 [03:05<6:17:37,  4.84s/it]
  1%|          | 39/4724 [03:10<6:21:36,  4.89s/it]
  1%|          | 40/4724 [03:15<6:22:40,  4.90s/it]
  1%|          | 41/4724 [03:20<6:24:29,  4.93s/it]
  1%|          | 42/4724 [03:25<6:19:39,  4.87s/it]
  1%|          | 43/4724 [03:30<6:20:30,  4.88s/it]
  1%|          | 44/4724 [03:35<6:17:30,  4.84s/it]
  1%|          | 45/4724 [03:40<6:27:03,  4.96s/it]
  1%|          | 46/4724 [03:45<6:22:19,  4.90s/it]
  1%|          | 47/4724 [03:49<6:17:35,  4.84s/it]
  1%|          | 48/4724 [03:54<6:19:45,  4.87s/it]
  1%|          | 49/4724 [03:59<6:22:34,  4.91s/it]
  1%|          | 50/4724 [04:04<6:25:24,  4.95s/it]
  1%|          | 51/4724 [04:10<6:29:24,  5.00s/it]
  1%|          | 52/4724 [04:14<6:25:48,  4.95s/it]
  1%|          | 53/4724 [04:19<6:17:14,  4.85s/it]
  1%|          | 54/4724 [04:24<6:14:40,  4.81s/it]
  1%|          | 55/4724 [04:29<6:16:42,  4.84s/it]
  1%|          | 56/4724 [04:33<6:10:42,  4.76s/it]
  1%|          | 57/4724 [04:38<6:07:21,  4.72s/it]
  1%|          | 58/4724 [04:42<6:05:08,  4.70s/it]
  1%|          | 59/4724 [04:47<6:02:37,  4.66s/it]
  1%|▏         | 60/4724 [04:52<6:04:59,  4.70s/it]
  1%|▏         | 61/4724 [04:57<6:05:22,  4.70s/it]
  1%|▏         | 62/4724 [05:01<6:09:31,  4.76s/it]
  1%|▏         | 63/4724 [05:06<6:10:38,  4.77s/it]
  1%|▏         | 64/4724 [05:11<6:16:17,  4.84s/it]
  1%|▏         | 65/4724 [05:16<6:15:01,  4.83s/it]
  1%|▏         | 66/4724 [05:21<6:06:31,  4.72s/it]
  1%|▏         | 67/4724 [05:25<6:07:51,  4.74s/it]
  1%|▏         | 68/4724 [05:30<6:05:09,  4.71s/it]  1%|▏         | 69/4724 [05:35<6:03:40,  4.69s/it]