{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ae698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass, field, fields    ## For TrlParser\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    set_seed\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig, TrlParser, setup_chat_format\n",
    "from peft import LoraConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ed22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"\n",
    "    함수 실행 시간 출력\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import datetime\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "\n",
    "        sec = end - start\n",
    "        worktime = str(datetime.timedelta(seconds=sec)).split(\".\")[0]\n",
    "        print(f\"Working Time: {worktime}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64dfe34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\"json\", data_files = \"train_dataset.json\", split = \"train\")\n",
    "test_ds = load_dataset(\"json\", data_files = \"test_dataset.json\", split = \"train\")\n",
    "\n",
    "## 토크나이저 로드 및 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    use_fast = True,            ## Rust로 구현된 Fast Tokenizer 사용 (Qwen, RoPE, ChatGLM 등의 특이한 구조에서는 호환 안됨)\n",
    "    trust_remote_code = True)   ## 모델 코드 전체 다운로드 후 사용\n",
    "tokenizer.pad_token = tokenizer.eos_token       ## 패딩할 토큰 설정\n",
    "tokenizer.padding_side = \"left\"                 ## 디코더이므로 왼쪽을 패딩 (마지막 토큰을 보고 생성)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe20ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + eos_token }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "            \"{% generation %}\"\n",
    "            \"{{ message['content'] +  eos_token }}\"\n",
    "            \"{% endgeneration %}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{%- if add_generation_prompt %}\"\n",
    "    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n",
    "    \"{%- endif %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb7cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67db7ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.',\n",
       "  'role': 'system'},\n",
       " {'content': '생선찌개를 비린내 없이 끓이는 방법은 무엇인가요?', 'role': 'user'},\n",
       " {'content': '생선찌개를 맛있게 끓이는 방법으로 비린내를 없애는 방법이 있습니다. 아래 방법들을 참고해보세요. \\n\\n- 비린내가 많이 나는 생선찌개에 마지막으로 식초를 넣으면 비린내가 없어집니다. 또한, 생선을 구울 때 껍질에 식초를 바르면 껍질이 벗겨지지 않고 제 모양대로 구울 수 있습니다. \\n- 생선찌개를 만들 때 생선이 다 익은 다음 된장을 풀어 넣으면 비린내를 없앨 수 있습니다. \\n- 깨끗이 손질한 생선이라도 미처 손질하지 못한 잡티가 붙어 있을 수 있는데, 끓이기 전에 팔팔 끓는 물을 살짝 끼얹으면 비린내도 가시고 국물이 깔끔합니다. \\n- 간을 한 국물이 한참 끓으면 그때 생선을 넣습니다. \\n\\n위 방법을 참고해서 집에서 맛있는 비린내 없는 생선찌개를 만들어 보세요.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "299ab702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "생선찌개를 비린내 없이 끓이는 방법은 무엇인가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "생선찌개를 맛있게 끓이는 방법으로 비린내를 없애는 방법이 있습니다. 아래 방법들을 참고해보세요. \n",
      "\n",
      "- 비린내가 많이 나는 생선찌개에 마지막으로 식초를 넣으면 비린내가 없어집니다. 또한, 생선을 구울 때 껍질에 식초를 바르면 껍질이 벗겨지지 않고 제 모양대로 구울 수 있습니다. \n",
      "- 생선찌개를 만들 때 생선이 다 익은 다음 된장을 풀어 넣으면 비린내를 없앨 수 있습니다. \n",
      "- 깨끗이 손질한 생선이라도 미처 손질하지 못한 잡티가 붙어 있을 수 있는데, 끓이기 전에 팔팔 끓는 물을 살짝 끼얹으면 비린내도 가시고 국물이 깔끔합니다. \n",
      "- 간을 한 국물이 한참 끓으면 그때 생선을 넣습니다. \n",
      "\n",
      "위 방법을 참고해서 집에서 맛있는 비린내 없는 생선찌개를 만들어 보세요.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(train_ds[0][\"messages\"], tokenize = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd84b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "## 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,                    ## 4비트 양자화\n",
    "    bnb_4bit_use_double_quant = True,       ## 추가 양자화로 성능 손실 없이 파라미터당 0.4bit 추가 절약\n",
    "    bnb_4bit_quant_type = \"nf4\",            ## 양자화 데이터 타입 지정: 4비트 기반 모델 훈련 시 사용\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 ## Llama-3.1-8B의 학습 자료형. 저장은 4비트지만, 계산은 양자화 없이\n",
    ")\n",
    "\n",
    "## 모델 로드 및 설정\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    device_map = \"cuda:0\",\n",
    "    use_cache = False,                          ## VRAM 캐시 미사용, 추론 속도 저하. gradienc_checkpointing과 동시 사용 불가\n",
    "    low_cpu_mem_usage = True,                   ## CPU RAM 사용량 적게...\n",
    "    attn_implementation = \"flash_attention_2\",  ## flash_attention 연산 사용\n",
    "    quantization_config = bnb_config,\n",
    "    dtype = torch.bfloat16                      ## Llama-3.1-8B의 자료형으로 설정\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac310653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_and_masking(example):\n",
    "    all_input_ids = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i, message in enumerate(example[\"messages\"]):\n",
    "        current_messages = [message]\n",
    "        encoded_tensor = tokenizer.apply_chat_template(\n",
    "            current_messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = False,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_tensor.squeeze(0).tolist()\n",
    "\n",
    "        # print(f\"Original Message: {current_messages}\")\n",
    "        # print(f\"Apply chat template: {input_ids}\")\n",
    "\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            labels = list(input_ids)           ## 생성부는 그대로\n",
    "        else:\n",
    "            labels = [-100] * len(input_ids)   ## system, user는 마스킹\n",
    "\n",
    "        # print(f\"After Masking: {labels}\")\n",
    "\n",
    "        all_input_ids.extend(input_ids)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"labels\": all_labels,\n",
    "        \"attention_mask\": [1]*len(all_input_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72888bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Packing train dataset: 100%|██████████| 19039/19039 [00:00<00:00, 27733.41 examples/s]\n",
      "Packing eval dataset: 100%|██████████| 2116/2116 [00:00<00:00, 26918.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r = 64,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    max_length = 1024,\n",
    "    output_dir = \"./results/assistant-only-4-epoch\",\n",
    "    report_to = \"none\",\n",
    "    assistant_only_loss = True,\n",
    "    learning_rate = 5e-5,\n",
    "    lr_scheduler_type = \"cosine_with_restarts\",\n",
    "    lr_scheduler_kwargs = {\"num_cycles\": 3},\n",
    "    num_train_epochs = 4,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 100,\n",
    "    save_strategy = \"epoch\",\n",
    "    weight_decay = 0.01,\n",
    "    max_grad_norm = 0.5,\n",
    "    warmup_ratio = 0.03,\n",
    "    bf16 = True,\n",
    "    tf32 = True,\n",
    "    gradient_checkpointing = True,\n",
    "    packing = True,\n",
    "    dataloader_num_workers = 4,\n",
    "    push_to_hub = True,\n",
    "    dataset_kwargs = {\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    processing_class = tokenizer,\n",
    "    peft_config = peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a7fdd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainer.train_dataset))[\"assistant_masks\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c3c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer의 collator 가져오기\n",
    "collator = trainer.data_collator\n",
    "\n",
    "# 원본 샘플 하나를 collate 시도\n",
    "batch = collator([trainer.train_dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0acaca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "과일을 이용한 피부 미용은 어떤 것이 있는지 알려주세요. 각 과일별로 어떻게 활용할 수 있고, 어디에 좋은 효과가 있는지도 알려주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "과일로 할 수 있는 피부 미용은 매우 다양합니다. 각 과일마다 그에 맞는 방법으로 활용할 수 있습니다. 따라서 각 과일에 대한 정보와 활용법을 알아보겠습니다.\n",
      "\n",
      "- 레몬: 피부를 맑고 희게하고 모세혈관을 튼튼하게 해 기미, 주근깨, 빨간 뾰루지 등에 좋습니다. 산도가 강하므로 다른 재료와 섞어 사용하는 것이 좋습니다. 레몬즙 1작은술에 우유 약간, 해조 가루 약간을 섞어 부드럽게 만든 후 사용합니다.\n",
      "\n",
      "- 키위: 비타민 C가 풍부해서 미백 효과가 뛰어나며, 당분, 무기질, 미네랄, 철분 등도 풍부해 탄력을 좋게 하고 피부의 수분 함유량을 높여 줍니다. 키위 반개를 갈아 오트밀 가루를 넣고 걸쭉해지면 사용합니다.\n",
      "\n",
      "- 바나나: 보습 효과가 뛰어나며 메마른 피부에 좋습니다. 환절기 건조 피부에도 좋으며, 잔주름이 잘 생기는 악건성 피부에도 사용할 수 있습니다. 으깬 바나나 1큰술에 달걀 노른자와 밀가루를 섞어 사용합니다.\n",
      "\n",
      "- 딸기: 피부를 희게 하는 비타민 C와 각질을 잘 떨어져 나가도록 하는 과일산이 풍부하여 여드름 피부, 기미, 주근깨에 효과가 있습니다. 으깬 딸기에 플레인 요구르트와 오트밀을 넣어 걸쭉하게 만든 후 사용합니다.\n",
      "\n",
      "- 토마토: 유기산과 비타민 A, C 등의 영양소가 풍부하고 항균 작용이 뛰어나서 피지가 많은 지성 피부, 거친 피부, 여드름 피부, 블랙 헤드를 제거하는 데 효과적입니다. 싱싱한 토마토를 강판에 갈아 맥반석 가루를 넣어 걸쭉하게 만들어 사용합니다.\n",
      "\n",
      "- 사과: 피부 탄력을 높여주어 거칠어진 건성 피부의 회복에 도움을 주고, 피부를 투명하고 매끄럽게 해줍니다. 사과산, 비타민, 당분이 풍부해 피부 수분 함유량을 높여 줍니다. 싱싱한 사과를 강판에 갈아 밀가루나 해조 가루를 섞어 만듭니다.\n",
      "\n",
      "- 오렌지: 피부를 건강하고 하얗게 만들어주며 모세혈관을 튼튼하게 해줍니다. 건조하고 거칠어진 피부, 기미, 주근깨에 효과적입니다. 오렌지즙 1큰술에 해조가루 1/2 작은술, 그리고 물을 약간 섞어 만듭니다. \n",
      "\n",
      "- 청포도: 과일산이 풍부하여 두꺼워진 피부 각질을 정상적으로 회복시켜 부드러운 살각을 만들어주고 피부 보습에도 좋습니다. 청보도를 잘 으깬 후 영양크림과 꿀을 약간 섞어 사용합니다.\n",
      "\n",
      "위와 같은 방법들이 있으며, 레몬처럼 산성이 강한 과일이나, 청포도처럼 알러지가 있는 경우 사용에 주의해야 합니다. 이 외에도 여러 가지 과일을 이용한 방법들이 있으니 찾아보시면 좋습니다.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(trainer.train_dataset))[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e3b319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "당신은 다양한 분야의 전문가들이 제공한 지식과 정보를 바탕으로 만들어진 AI 어시스턴트입니다. 사용자들의 질문에 대해 정확하고 유용한 답변을 제공하는 것이 당신의 주요 목표입니다. 복잡한 주제에 대해서도 이해하기 쉽게 설명할 수 있으며, 필요한 경우 추가 정보나 관련 예시를 제공할 수 있습니다. 항상 객관적이고 중립적인 입장을 유지하면서, 최신 정보를 반영하여 답변해 주세요. 사용자의 질문이 불분명한 경우 추가 설명을 요청하고, 당신이 확실하지 않은 정보에 대해서는 솔직히 모른다고 말해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "토성의 고리가 빛의 띠로 보이는 이유는 무엇인가요?  \n",
      "\n",
      "토성의 고리는 얼음과 같은 여러 물질로 이루어져 있다고 알고 있는데, 카시니가 찍은 사진에서 마치 빛의 띠 처럼 보이는 이유가 무엇인가요? 물질의 공전 속도가 빠르기 때문에 카메라로 담았을 때 빛의 궤적으로 보이는 건가요? 또한, 야간에 빠르게 움직이는 자동차를 장노출로 찍었을 때 빛의 궤적이 생기는 것과 같은 원리일까요? 그리고 빛의 궤적이 생기는 것은 우주라는 어두운 환경 특성 때문이라고 생각됩니다. 이게 맞을까요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "토성의 고리가 미세한 입자들로 이루어져 있기 때문에, 입자들의 밀도 차이 때문에 카시니 탐사선에서 찍은 고해상도 사진에서 빛의 띠가 보이는 것입니다.  \n",
      "\n",
      "실제로는 토성의 고리 입자들의 운동이 장노출 사진에서 잔상이 생기는 이유와 관련이 없습니다. 물체의 운동은 토성의 고리가 매끄럽게 보이는 이유와 상관이 없습니다. \n",
      "\n",
      "밀도 차이로 생긴 미세한 입자들의 밀도는 연속적인 것이 아니며 광학계의 분해능으로 인해 고해상도 사진에서 입자 간격이 잘 보이지 않습니다. 따라서, 토성의 고리가 빛의 띠로 보이는\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(next(iter(trainer.train_dataset))[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a71d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
