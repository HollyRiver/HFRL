{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb894f0",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import os\n",
    "\n",
    "from trl import DPOTrainer\n",
    "from huggingface_hub import login\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff1cd2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "* LLM을 통한 Feedback 자동 수집 자료 https://arxiv.org/abs/2310.01377\n",
    "* Implicit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78938f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "## 원시 데이터 로드\n",
    "ds = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b7447c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 28/28 [00:01<00:00, 15.83ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 19.12ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 28/28 [00:03<00:00,  8.52ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 10.96ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10251164"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_split = ds[\"train\"].train_test_split(test_size = 0.5, seed = 42)\n",
    "\n",
    "## For SFT\n",
    "sft_ds = ds_split[\"train\"]\n",
    "sft_ds = sft_ds.rename_column(\"chosen\", \"messages\").remove_columns([col for col in sft_ds.column_names if col != \"chosen\"]).train_test_split(test_size = 0.1, seed = 42)\n",
    "sft_ds[\"train\"].to_json(\"./data/sft_train_dataset.json\", orient = \"records\")\n",
    "sft_ds[\"test\"].to_json(\"./data/sft_test_dataset.json\", orient = \"records\")\n",
    "\n",
    "## Implicit Prompt -> Explicit Prompt\n",
    "dpo_ds = ds_split[\"test\"].map(\n",
    "    lambda sample: {\n",
    "        \"prompt\": [{\"role\": \"user\", \"content\": sample[\"prompt\"]}],\n",
    "        \"chosen\": [content for content in sample[\"chosen\"] if content[\"role\"] == \"assistant\"],\n",
    "        \"rejected\": [content for content in sample[\"rejected\"] if content[\"role\"] == \"assistant\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "dpo_ds = dpo_ds.remove_columns([col for col in dpo_ds.column_names if col not in [\"prompt\", \"chosen\", \"rejected\"]]).train_test_split(test_size = 0.1, seed = 42)\n",
    "dpo_ds[\"train\"].to_json(\"./data/dpo_train_dataset.json\", orient = \"records\")\n",
    "dpo_ds[\"test\"].to_json(\"./data/dpo_test_dataset.json\", orient = \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935eda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard format\n",
    "## Explicit prompt (recommended)\n",
    "preference_example = {\"prompt\": \"The sky is\", \"chosen\": \" blue.\", \"rejected\": \" green.\"}\n",
    "# Implicit prompt\n",
    "preference_example = {\"chosen\": \"The sky is blue.\", \"rejected\": \"The sky is green.\"}\n",
    "\n",
    "# Conversational format\n",
    "## Explicit prompt (recommended)\n",
    "preference_example = {\"prompt\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"}],\n",
    "                      \"chosen\": [{\"role\": \"assistant\", \"content\": \"It is blue.\"}],\n",
    "                      \"rejected\": [{\"role\": \"assistant\", \"content\": \"It is green.\"}]}\n",
    "## Implicit prompt\n",
    "preference_example = {\"chosen\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"},\n",
    "                                 {\"role\": \"assistant\", \"content\": \"It is blue.\"}],\n",
    "                      \"rejected\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"},\n",
    "                                   {\"role\": \"assistant\", \"content\": \"It is green.\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb462d7",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4103aea9",
   "metadata": {},
   "source": [
    "* 일단 기존 방식대로 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f6c130",
   "metadata": {},
   "source": [
    "## DPO\n",
    "\n",
    "DPOTrainer 소스 코드 https://github.com/huggingface/trl/blob/d625c5533a6b1c84d3565c8080857f6bb81c538a/trl/trainer/dpo_trainer.py#L1145-L1149\n",
    "\n",
    "* SFT보다 learning_rate를 훨씬 작게 설정해줘야 함\n",
    "* packing은 쌍으로 존재하는 데이터에서 불가능. padding_free는 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d48a0d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a ManyToManyField in Django, you need to define a ManyToManyField on both models involved in the relationship.\n",
      "\n",
      "Assuming you already have the models for which you want to create a many-to-many relationship, you can add the ManyToManyField as follows:\n",
      "\n",
      "1. In the model where you want to define the many-to-many field, add the following line of code:\n",
      "```python\n",
      "class MyModel(models.Model):\n",
      "    # fields and other descriptors for MyModel\n",
      "    related_models = models.ManyToManyField('OtherModel', related_name='my_model_set')\n",
      "```\n",
      "Replace 'OtherModel' with the name of the model you want to create a many-to-many relationship with, and'my\\_model\\_set' with the name you want to use for the reverse relationship on the other model.\n",
      "\n",
      "2. In the model where you want to define the reverse many-to-many field, add the following line of code:\n",
      "```python\n",
      "class OtherModel(models.Model):\n",
      "    # fields and other descriptors for OtherModel\n",
      "    my_model = models.ManyToManyField('MyModel', related_name='other_model_set')\n",
      "```\n",
      "This creates a many-to-many relationship between the two models, and you can use the field in your model's administration page to manage the relationship.<|eot_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dpo_trainer.train_dataset[0][\"chosen_input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2aa41",
   "metadata": {},
   "source": [
    "## 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ede2d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc696f",
   "metadata": {},
   "source": [
    "`-` 테스트에 사용될 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b61de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"json\", data_files = os.path.join(\"\", \"data/dpo_test_dataset.json\"), split = \"train\")\n",
    "random_idx = randint(0, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b057feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In this task you will be given a list of numbers. You should remove any number that is not an integer (whole number). If every number is not an whole number then an empty list (\"[]\") should be returned. Otherwise, answer with the list of whole numbers separated by comma inside brackets.\n",
      "\n",
      "Example Input: [-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]\n",
      "Example Output: [-71, -72, -81, -54]\n",
      "\n",
      "Example Input: [57, -7.215, 86, 28, -14.904, 40.471, -10.607]\n",
      "Example Output: [57, 86, 28]\n",
      "\n",
      "Example Input: [19.171, -24.194, -31, 93, 22.236, 13]\n",
      "Example Output:\n",
      "\n",
      "\n",
      "chosen: [-31, 93, 13]\n",
      "\n",
      "\n",
      "rejected: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in test_dataset[random_idx].items():\n",
    "    print(f\"{k}: {v[0][\"content\"]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68ac67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = test_dataset[random_idx][\"prompt\"]\n",
    "chosen = test_dataset[random_idx][\"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac172034",
   "metadata": {},
   "source": [
    "`-` 원시 모델 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd0f7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "origin_model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "origin_model = AutoModelForCausalLM.from_pretrained(origin_model_name, use_cache = False, device_map = \"cuda:0\", dtype = torch.bfloat16)\n",
    "origin_tokenizer = AutoTokenizer.from_pretrained(origin_model_name, use_fast = True)\n",
    "origin_tokenizer.pad_token = origin_tokenizer.eos_token\n",
    "origin_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4515a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "In this task you will be given a list of numbers. You should remove any number that is not an integer (whole number). If every number is not an whole number then an empty list (\"[]\") should be returned. Otherwise, answer with the list of whole numbers separated by comma inside brackets.\n",
      "\n",
      "Example Input: [-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]\n",
      "Example Output: [-71, -72, -81, -54]\n",
      "\n",
      "Example Input: [57, -7.215, 86, 28, -14.904, 40.471, -10.607]\n",
      "Example Output: [57, 86, 28]\n",
      "\n",
      "Example Input: [19.171, -24.194, -31, 93, 22.236, 13]\n",
      "Example Output:\n",
      "\n",
      "chosen:\n",
      "[-31, 93, 13]\n",
      "\n",
      "generate:\n",
      "### Filtering Whole Numbers from a List\n",
      "\n",
      "Here's a Python function that filters out non-integer numbers from a given list:\n",
      "\n",
      "```python\n",
      "def filter_whole_numbers(numbers):\n",
      "    \"\"\"\n",
      "    Returns a list of whole numbers from the input list.\n",
      "    If no whole numbers are found, an empty list is returned.\n",
      "    \n",
      "    Args:\n",
      "        numbers (list): A list of numbers.\n",
      "    \n",
      "    Returns:\n",
      "        list: A list of whole numbers.\n",
      "    \"\"\"\n",
      "    whole_numbers = [num for num in numbers if num == int(num)]\n",
      "    return whole_numbers if whole_numbers else []\n",
      "\n",
      "# Example usage:\n",
      "print(filter_whole_numbers([-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]))\n",
      "# Output: [-71, -72, -81, -54]\n",
      "\n",
      "print(filter_whole_numbers([57, -7.215, 86, 28, -14.904, 40.471, -10.607]))\n",
      "# Output: [57, 86, 28]\n",
      "\n",
      "print(filter_whole_numbers([19.171, -24.194, -31, 93, 22.236, 13]))\n",
      "# Output: [-31, 93, 13]\n",
      "```\n",
      "\n",
      "This function uses a list comprehension to iterate over the input list and check each number against its integer equivalent. If the number is equal to its integer value (i.e., it's a whole number), it's included in the `whole_numbers` list. Finally, if the `whole_numbers` list is not empty, it's returned; otherwise, an empty list is returned.\n"
     ]
    }
   ],
   "source": [
    "terminators = [origin_tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = origin_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(origin_model.device)\n",
    "\n",
    "outputs = origin_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{origin_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1df70337",
   "metadata": {},
   "outputs": [],
   "source": [
    "del origin_model\n",
    "del origin_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69744d39",
   "metadata": {},
   "source": [
    "`-` 파인튜닝 후 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad71ffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "SFT_model_name = \"./results/test\"\n",
    "\n",
    "SFT_model = AutoModelForCausalLM.from_pretrained(SFT_model_name, use_cache = False, device_map = \"cuda:0\", dtype = torch.bfloat16)\n",
    "SFT_tokenizer = AutoTokenizer.from_pretrained(SFT_model_name, use_fast = True)\n",
    "SFT_tokenizer.pad_token = SFT_tokenizer.eos_token\n",
    "SFT_tokenizer.pad_token_id = SFT_tokenizer.eos_token_id\n",
    "SFT_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "045302ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "In this task you will be given a list of numbers. You should remove any number that is not an integer (whole number). If every number is not an whole number then an empty list (\"[]\") should be returned. Otherwise, answer with the list of whole numbers separated by comma inside brackets.\n",
      "\n",
      "Example Input: [-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]\n",
      "Example Output: [-71, -72, -81, -54]\n",
      "\n",
      "Example Input: [57, -7.215, 86, 28, -14.904, 40.471, -10.607]\n",
      "Example Output: [57, 86, 28]\n",
      "\n",
      "Example Input: [19.171, -24.194, -31, 93, 22.236, 13]\n",
      "Example Output:\n",
      "\n",
      "chosen:\n",
      "[-31, 93, 13]\n",
      "\n",
      "generate:\n",
      "[93, 13]\n"
     ]
    }
   ],
   "source": [
    "terminators = [SFT_tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = SFT_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(SFT_model.device)\n",
    "\n",
    "outputs = SFT_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{SFT_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafafcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "In this task you will be given a list of numbers. You should remove any number that is not an integer (whole number). If every number is not an whole number then an empty list (\"[]\") should be returned. Otherwise, answer with the list of whole numbers separated by comma inside brackets.\n",
      "\n",
      "Example Input: [-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]\n",
      "Example Output: [-71, -72, -81, -54]\n",
      "\n",
      "Example Input: [57, -7.215, 86, 28, -14.904, 40.471, -10.607]\n",
      "Example Output: [57, 86, 28]\n",
      "\n",
      "Example Input: [19.171, -24.194, -31, 93, 22.236, 13]\n",
      "Example Output:\n",
      "\n",
      "chosen:\n",
      "[-31, 93, 13]\n",
      "\n",
      "generate:\n",
      "[-31, 93, 13]\n"
     ]
    }
   ],
   "source": [
    "outputs = SFT_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"generate:\\n{SFT_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b87615ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "del SFT_model\n",
    "del SFT_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7c2bd",
   "metadata": {},
   "source": [
    "`-` DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "dpo_model_name = \"./results/dpo-vanilla\"\n",
    "\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(dpo_model_name, use_cache = False, device_map = \"cuda:0\", dtype = torch.bfloat16)\n",
    "dpo_tokenizer = AutoTokenizer.from_pretrained(dpo_model_name, use_fast = True)\n",
    "dpo_tokenizer.pad_token = dpo_tokenizer.eos_token\n",
    "dpo_tokenizer.pad_token_id = dpo_tokenizer.eos_token_id\n",
    "dpo_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "[{'content': 'In this task you will be given a list of numbers. You should remove any number that is not an integer (whole number). If every number is not an whole number then an empty list (\"[]\") should be returned. Otherwise, answer with the list of whole numbers separated by comma inside brackets.\\n\\nExample Input: [-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]\\nExample Output: [-71, -72, -81, -54]\\n\\nExample Input: [57, -7.215, 86, 28, -14.904, 40.471, -10.607]\\nExample Output: [57, 86, 28]\\n\\nExample Input: [19.171, -24.194, -31, 93, 22.236, 13]\\nExample Output:', 'role': 'user'}]\n",
      "\n",
      "chosen:\n",
      "[{'content': '[-31, 93, 13]', 'role': 'assistant'}]\n",
      "\n",
      "생성답변:\n",
      "Here is a Python solution for this task:\n",
      "\n",
      "```python\n",
      "def filter_integers(numbers):\n",
      "    \"\"\"\n",
      "    This function filters out non-integer numbers from a list.\n",
      "    \n",
      "    Args:\n",
      "    numbers (list): A list of numbers.\n",
      "    \n",
      "    Returns:\n",
      "    list: A list of integers separated by comma inside brackets if every number in the list is an integer, otherwise an empty list.\n",
      "    \"\"\"\n",
      "    \n",
      "    # Check if all numbers in the list are integers\n",
      "    if all(isinstance(num, int) for num in numbers):\n",
      "        # If all numbers are integers, return the list of integers as a string\n",
      "        return '[' + ', '.join(map(str, numbers)) + ']'\n",
      "    else:\n",
      "        # If not all numbers are integers, return an empty list\n",
      "        return '[]'\n",
      "\n",
      "# Example usage:\n",
      "print(filter_integers([-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]))\n",
      "print(filter_integers([57, -7.215, 86, 28, -14.904, 40.471, -10.607]))\n",
      "print(filter_integers([19.171, -24.194, -31, 93, 22.236, 13]))\n",
      "```\n",
      "\n",
      "This code defines a function `filter_integers` that takes a list of numbers as input and returns a list of integers as a string if every number in the list is an integer, otherwise it returns an empty list. The `isinstance(num, int)` function is used to check if a number is an integer. The `all()` function is used with a generator expression to check if all numbers in the list are integers. If all numbers are integers, the list of integers is converted to a string with commas in between each number and enclosed in brackets.\n"
     ]
    }
   ],
   "source": [
    "terminators = [dpo_tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = dpo_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(dpo_model.device)\n",
    "\n",
    "outputs = dpo_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages[0][\"content\"]}\\n\")\n",
    "print(f\"chosen:\\n{chosen[0][\"content\"]}\\n\")\n",
    "print(f\"생성답변:\\n{dpo_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd962f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dpo_model\n",
    "del dpo_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc024193",
   "metadata": {},
   "source": [
    "`-` APO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "943e5e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "apo_model_name = \"./results/dpo-test\"\n",
    "\n",
    "apo_model = AutoModelForCausalLM.from_pretrained(apo_model_name, use_cache = False, device_map = \"cuda:0\", dtype = torch.bfloat16)\n",
    "apo_tokenizer = AutoTokenizer.from_pretrained(apo_model_name, use_fast = True)\n",
    "apo_tokenizer.pad_token = apo_tokenizer.eos_token\n",
    "apo_tokenizer.pad_token_id = apo_tokenizer.eos_token_id\n",
    "apo_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ddd1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "[{'content': 'In this task you will be given a list of numbers. You should remove any number that is not an integer (whole number). If every number is not an whole number then an empty list (\"[]\") should be returned. Otherwise, answer with the list of whole numbers separated by comma inside brackets.\\n\\nExample Input: [-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]\\nExample Output: [-71, -72, -81, -54]\\n\\nExample Input: [57, -7.215, 86, 28, -14.904, 40.471, -10.607]\\nExample Output: [57, 86, 28]\\n\\nExample Input: [19.171, -24.194, -31, 93, 22.236, 13]\\nExample Output:', 'role': 'user'}]\n",
      "\n",
      "chosen:\n",
      "[{'content': '[-31, 93, 13]', 'role': 'assistant'}]\n",
      "\n",
      "생성답변:\n",
      "Here is a Python function that accomplishes this task:\n",
      "\n",
      "```python\n",
      "def remove_floats(numbers):\n",
      "    \"\"\"\n",
      "    This function removes any number that is not an integer from the input list.\n",
      "    \n",
      "    If every number is not an integer, it returns an empty list.\n",
      "    \n",
      "    Otherwise, it returns the list of integers separated by comma inside brackets.\n",
      "    \n",
      "    Parameters:\n",
      "    numbers (list): A list of numbers\n",
      "    \n",
      "    Returns:\n",
      "    list: A list of integers or an empty list\n",
      "    \"\"\"\n",
      "    \n",
      "    # Use a list comprehension to filter out non-integer numbers\n",
      "    whole_numbers = [num for num in numbers if num == int(num)]\n",
      "    \n",
      "    # If the list of whole numbers is empty, return an empty list\n",
      "    if not whole_numbers:\n",
      "        return []\n",
      "    \n",
      "    # Otherwise, return the list of whole numbers as a string with comma separation\n",
      "    return '[' + ', '.join(map(str, whole_numbers)) + ']'\n",
      "\n",
      "# Test the function\n",
      "print(remove_floats([-71, -72, -74.582, -81, 45.11, 19.803, 39.621, -1.112, -11.601, -54]))\n",
      "# Output: [-71, -72, -81, -54]\n",
      "\n",
      "print(remove_floats([57, -7.215, 86, 28, -14.904, 40.471, -10.607]))\n",
      "# Output: [57, 86, 28]\n",
      "\n",
      "print(remove_floats([19.171, -24.194, -31, 93, 22.236, 13]))\n",
      "# Output: [-31, 13, 93]\n",
      "```\n",
      "\n",
      "This function uses a list comprehension to filter out non-integer numbers. It then checks if the resulting list is empty. If it is, it returns an empty list. Otherwise, it returns the list of integers as a string with comma separation.\n"
     ]
    }
   ],
   "source": [
    "terminators = [apo_tokenizer.eos_token_id]\n",
    "\n",
    "input_ids = apo_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,   ## 생성 시에 맨 뒤 generation 시작하라는 프롬프트 삽입\n",
    "    return_tensors = \"pt\").to(apo_model.device)\n",
    "\n",
    "outputs = apo_model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens = 512,       ## prompt 제외 출력 토큰 수. max_length는 prompt 포함\n",
    "    eos_token_id = terminators,\n",
    "    do_sample = True,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.95\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(f\"prompt:\\n{messages}\\n\")\n",
    "print(f\"chosen:\\n{chosen}\\n\")\n",
    "print(f\"생성답변:\\n{apo_tokenizer.decode(response, skip_special_tokens = True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "799c290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del apo_model\n",
    "del apo_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c4190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
