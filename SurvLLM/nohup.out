Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
  0%|          | 0/200 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 1/200 [00:24<1:20:28, 24.26s/it]  1%|          | 2/200 [00:38<1:01:32, 18.65s/it]  2%|▏         | 3/200 [00:53<54:44, 16.67s/it]    2%|▏         | 4/200 [01:11<56:09, 17.19s/it]  2%|▎         | 5/200 [01:26<53:17, 16.40s/it]  3%|▎         | 6/200 [01:41<52:09, 16.13s/it]  4%|▎         | 7/200 [01:57<51:32, 16.02s/it]  4%|▍         | 8/200 [02:15<53:26, 16.70s/it]  4%|▍         | 9/200 [02:32<53:24, 16.78s/it]  5%|▌         | 10/200 [02:49<53:16, 16.82s/it]  6%|▌         | 11/200 [03:04<50:58, 16.18s/it]  6%|▌         | 12/200 [03:21<51:58, 16.59s/it]  6%|▋         | 13/200 [03:39<52:52, 16.97s/it]  7%|▋         | 14/200 [03:56<52:37, 16.98s/it]Traceback (most recent call last):
  File "/root/HFRL/SurvLLM/sft_generate.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:07,  3.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.96s/it]
  0%|          | 0/200 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 1/200 [00:22<1:14:27, 22.45s/it]  1%|          | 2/200 [00:35<54:57, 16.65s/it]    2%|▏         | 3/200 [00:49<51:26, 15.67s/it]  2%|▏         | 4/200 [01:03<49:04, 15.02s/it]  2%|▎         | 5/200 [01:17<48:07, 14.81s/it]  3%|▎         | 6/200 [01:33<48:17, 14.94s/it]  4%|▎         | 7/200 [01:46<46:38, 14.50s/it]  4%|▍         | 8/200 [02:01<46:24, 14.50s/it]  4%|▍         | 9/200 [02:17<47:39, 14.97s/it]  5%|▌         | 10/200 [02:33<48:50, 15.42s/it]  6%|▌         | 11/200 [02:47<47:11, 14.98s/it]  6%|▌         | 12/200 [03:04<48:48, 15.58s/it]  6%|▋         | 13/200 [03:38<1:06:10, 21.23s/it]  7%|▋         | 14/200 [03:59<1:05:04, 20.99s/it]